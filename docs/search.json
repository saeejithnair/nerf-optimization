[
  {
    "objectID": "documentation/index.html",
    "href": "documentation/index.html",
    "title": "Documentation",
    "section": "",
    "text": "Documentation and walkthroughs explaining the design, motivation, and architecture."
  },
  {
    "objectID": "experiments/exploration/Preliminary_Basic_Sweep.html",
    "href": "experiments/exploration/Preliminary_Basic_Sweep.html",
    "title": "Preliminary Basic Sweep",
    "section": "",
    "text": "This experiment trained and evaluated 100 Gen-Nerf models on the Blender Synthetic Lego scene. The search swept across 4 different configurations\n\nCoarse field initial blockspecs size (either layer widths of LARGE_VARIABLE: [240, 248, 256] or SMALL_VARIABLE: [56, 60, 64])\nFine field initial blockspecs size (either layer widths of LARGE_VARIABLE: [240, 248, 256] or SMALL_VARIABLE: [56, 60, 64])\nCoarse field target ratio argument to DarwinAI SDK (pipeline.model.coarse-field.base-mlp.target-ratio: [0.1, 0.25, 0.5, 0.75, 1])\nFine field target ratio argument to DarwinAI SDK (pipeline.model.fine-field.base-mlp.target-ratio: [0.1, 0.25, 0.5, 0.75, 1])"
  },
  {
    "objectID": "experiments/exploration/Preliminary_Basic_Sweep.html#sweep-parameters-versus-similarity-metrics",
    "href": "experiments/exploration/Preliminary_Basic_Sweep.html#sweep-parameters-versus-similarity-metrics",
    "title": "Preliminary Basic Sweep",
    "section": "2.1 Sweep Parameters versus Similarity Metrics",
    "text": "2.1 Sweep Parameters versus Similarity Metrics\nAs expected based on our results from W’23, we see that the coarse field can be compressed down to almost zero (0.675%) of the original model size, and it does not hurt the quality of the model at all, as long as the fine field model is still big enough. In fact, it is very surprising that the best performing model is the one with the smallest coarse-field and largest fine field.\n\n\n\n\n\n\nWarning\n\n\n\nI would have expected the highest performing model to be closer to full-width for both coarse AND fine models. Or at the very least, if someone told me that the best performing model had the smallest coarse ratio, then I would have expected there to be some kind of monotonic-ish relationship between the coarse field ratio and similarity. Instead, a qualitative observation of the data suggests that there’s just a lot of variance, and there’s no clear relationship to be seen.\n\n\n\n\n\n    \n        \n        \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n    \n    \n    \n\n\n\n\n\n\n\n\nTip\n\n\n\nMost plots on this site are interactive! Don’t be afraid to click, drag, hover, full-screen, toggle, etc. Not all plots support every interaction but many of them have neat hidden features. E.g. try clicking on an axis in the above parallel coordinates plot and dragging up/down. It will highlight the runs which overlap with the selected range!"
  },
  {
    "objectID": "experiments/index.html",
    "href": "experiments/index.html",
    "title": "Experiments",
    "section": "",
    "text": "Results from experiments."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Vision and Image Processing Lab (VIP) @ UWaterloo, Department of Systems Design Engineering"
  },
  {
    "objectID": "reports/00_Setup_GenNerf.html",
    "href": "reports/00_Setup_GenNerf.html",
    "title": "Exploring the Vanilla NeRF Field + MLP",
    "section": "",
    "text": "This notebook does the following:\n\nLoads and evaluates the vanilla NeRF model which Eddy modified (W’23) to integrate with the DarwinAI SDK\nLoads and evaluates the new GenNerf model which we’ve built leveraging NerfStudio’s vanilla Nerf model\nShows how GenNerf can be integrated with the DarwinAI SDK"
  },
  {
    "objectID": "reports/00_Setup_GenNerf.html#coarse-model-architecture",
    "href": "reports/00_Setup_GenNerf.html#coarse-model-architecture",
    "title": "Exploring the Vanilla NeRF Field + MLP",
    "section": "2.1 Coarse Model Architecture",
    "text": "2.1 Coarse Model Architecture\n\n\nArchitecture summary for pytorch-nerf coarse model\ncoarse_model, fine_model, grad_vars, input_ch, input_ch_views = create_nerf()\nsummary(coarse_model)\n\n\n========================================================\nLayer (type (var_name):depth-idx)        Param #\n========================================================\nPyTorchNeRF (PyTorchNeRF)                --\n├─ModuleList (pts_linears): 1-1          --\n│    └─Linear (0): 2-1                   16,384\n│    └─Linear (1): 2-2                   65,792\n│    └─Linear (2): 2-3                   65,792\n│    └─Linear (3): 2-4                   65,792\n│    └─Linear (4): 2-5                   65,792\n│    └─Linear (5): 2-6                   81,920\n│    └─Linear (6): 2-7                   65,792\n│    └─Linear (7): 2-8                   65,792\n├─ModuleList (views_linears): 1-2        --\n│    └─Linear (0): 2-9                   36,352\n├─Linear (feature_linear): 1-3           65,792\n├─Linear (alpha_linear): 1-4             257\n├─Linear (rgb_linear): 1-5               387\n========================================================\nTotal params: 595,844\nTrainable params: 595,844\nNon-trainable params: 0\n========================================================"
  },
  {
    "objectID": "reports/00_Setup_GenNerf.html#coarse-field-base-mlp-architecture",
    "href": "reports/00_Setup_GenNerf.html#coarse-field-base-mlp-architecture",
    "title": "Exploring the Vanilla NeRF Field + MLP",
    "section": "2.2 Coarse Field Base MLP Architecture",
    "text": "2.2 Coarse Field Base MLP Architecture\nLet’s take a closer look at the model, especially the base MLP architecture (pts_linears), as it’s what we’ll be optimizing in GenNerf.\n\n\nDetailed architecture summary for pytorch-nerf coarse field base MLP\nsummary(coarse_model, input_size=(input_ch + input_ch_views,))\n\n\n========================================================================================================================\nLayer (type (var_name):depth-idx)        Input Shape      Output Shape     Param #          Param %          Mult-Adds\n========================================================================================================================\nPyTorchNeRF (PyTorchNeRF)                [90]             [4]              --                    --          --\n├─ModuleList (pts_linears): 1-1          --               --               --                    --          --\n│    └─Linear (0): 2-1                   [63]             [256]            16,384             2.75%          4,194,304\n│    └─Linear (1): 2-2                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (2): 2-3                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (3): 2-4                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (4): 2-5                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (5): 2-6                   [319]            [256]            81,920            13.75%          20,971,520\n│    └─Linear (6): 2-7                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (7): 2-8                   [256]            [256]            65,792            11.04%          16,842,752\n├─Linear (alpha_linear): 1-2             [256]            [1]              257                0.04%          257\n├─Linear (feature_linear): 1-3           [256]            [256]            65,792            11.04%          16,842,752\n├─ModuleList (views_linears): 1-4        --               --               --                    --          --\n│    └─Linear (0): 2-9                   [283]            [128]            36,352             6.10%          4,653,056\n├─Linear (rgb_linear): 1-5               [128]            [3]              387                0.06%          1,161\n========================================================================================================================\nTotal params: 595,844\nTrainable params: 595,844\nNon-trainable params: 0\nTotal mult-adds (M): 147.72\n========================================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.02\nParams size (MB): 2.38\nEstimated Total Size (MB): 2.40\n========================================================================================================================"
  },
  {
    "objectID": "reports/00_Setup_GenNerf.html#coarse-field-base-mlp-architecture-1",
    "href": "reports/00_Setup_GenNerf.html#coarse-field-base-mlp-architecture-1",
    "title": "Exploring the Vanilla NeRF Field + MLP",
    "section": "3.1 Coarse Field Base MLP Architecture",
    "text": "3.1 Coarse Field Base MLP Architecture\n\n\nDetailed architecture summary for VanillaModel coarse field base MLP from NerfStudio\nsummary(vanilla_nerf.field_coarse.mlp_base, input_size=(63,))\n\n\n========================================================================================================================\nLayer (type (var_name):depth-idx)        Input Shape      Output Shape     Param #          Param %          Mult-Adds\n========================================================================================================================\nMLP (MLP)                                [63]             [256]            --                    --          --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (0): 2-1                   [63]             [256]            16,384             3.32%          4,194,304\n├─ReLU (activation): 1-2                 [256]            [256]            --                    --          --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (1): 2-2                   [256]            [256]            65,792            13.34%          16,842,752\n├─ReLU (activation): 1-4                 [256]            [256]            --               (recursive)      --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (2): 2-3                   [256]            [256]            65,792            13.34%          16,842,752\n├─ReLU (activation): 1-6                 [256]            [256]            --               (recursive)      --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (3): 2-4                   [256]            [256]            65,792            13.34%          16,842,752\n├─ReLU (activation): 1-8                 [256]            [256]            --               (recursive)      --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (4): 2-5                   [319]            [256]            81,920            16.61%          20,971,520\n├─ReLU (activation): 1-10                [256]            [256]            --               (recursive)      --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (5): 2-6                   [256]            [256]            65,792            13.34%          16,842,752\n├─ReLU (activation): 1-12                [256]            [256]            --               (recursive)      --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (6): 2-7                   [256]            [256]            65,792            13.34%          16,842,752\n├─ReLU (activation): 1-14                [256]            [256]            --               (recursive)      --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (7): 2-8                   [256]            [256]            65,792            13.34%          16,842,752\n├─ReLU (out_activation): 1-16            [256]            [256]            --                    --          --\n========================================================================================================================\nTotal params: 493,056\nTrainable params: 493,056\nNon-trainable params: 0\nTotal mult-adds (M): 126.22\n========================================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.02\nParams size (MB): 1.97\nEstimated Total Size (MB): 1.99\n========================================================================================================================\n\n\n\n3.1.1 Discrepancy in the Base MLP Architecture between nerf-pytorch and NerfStudio\n\n\n\n\n\n\nPotential Bug?\n\n\n\nNote that there’s a discrepancy in the depth at which the skip connection is applied. In the nerf-pytorch implementation, the skip connection is applied at the 6th layer, whereas in NerfStudio’s implementation, the skip connection is applied at the 5th layer.\n\n\n\n\nTest case demonstrating discrepancy between pytorch-nerf and nerfstudio implementations\ncoarse_model, *rest = create_nerf()\n\nvanilla_pipeline_cfg = VanillaPipelineConfig()\nvanilla_nerf_cfg = VanillaModelConfig()\ndummy_datamanager = vanilla_pipeline_cfg.datamanager\nscene_box = SceneBox(\n    aabb=torch.tensor([[-1.5, -1.5, -1.5], [1.5, 1.5, 1.5]], dtype=torch.float32)\n)\nvanilla_nerf = vanilla_nerf_cfg.setup(\n    scene_box=scene_box,\n    num_train_data=100,\n)\n\ntest_module_lists_equal(\n    coarse_model.pts_linears, vanilla_nerf.field_coarse.mlp_base.layers\n)\n\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_807250/1772845051.py\", line 123, in test_module_lists_equal\n    assert module1.in_features == module2.in_features, (\nAssertionError: Module 4: Linear in_features differ (256 vs 319)\n\nList 1: ModuleList(\n  (0): Linear(in_features=63, out_features=256, bias=True)\n  (1): Linear(in_features=256, out_features=256, bias=True)\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): Linear(in_features=256, out_features=256, bias=True)\n  (4): Linear(in_features=256, out_features=256, bias=True)\n  (5): Linear(in_features=319, out_features=256, bias=True)\n  (6): Linear(in_features=256, out_features=256, bias=True)\n  (7): Linear(in_features=256, out_features=256, bias=True)\n)\nList 2: ModuleList(\n  (0): Linear(in_features=63, out_features=256, bias=True)\n  (1): Linear(in_features=256, out_features=256, bias=True)\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): Linear(in_features=256, out_features=256, bias=True)\n  (4): Linear(in_features=319, out_features=256, bias=True)\n  (5): Linear(in_features=256, out_features=256, bias=True)\n  (6): Linear(in_features=256, out_features=256, bias=True)\n  (7): Linear(in_features=256, out_features=256, bias=True)\n)"
  },
  {
    "objectID": "reports/00_Setup_GenNerf.html#creating-a-w23nerf-model",
    "href": "reports/00_Setup_GenNerf.html#creating-a-w23nerf-model",
    "title": "Exploring the Vanilla NeRF Field + MLP",
    "section": "4.1 Creating a W23NeRF model",
    "text": "4.1 Creating a W23NeRF model\nWe define a make_model_w23() function which creates a W23NeRF model in a manner that satisfies the darwinai.torch.builder.build_model() API and its model_fn argument. It takes in blockspecs which should map to the desired architecture of the original vanilla NeRF Field’s base MLP.\n\ndef make_model_w23(blockspecs: BlockSpecs, return_metadata=False):\n    INPUT_CH = 63\n    OUTPUT_CH = 5\n    SKIPS = [4]\n    INPUT_CH_VIEWS = 27\n    USE_VIEWDIRS = True\n\n    model = W23NeRF(\n        blockspecs,\n        input_ch=INPUT_CH,\n        output_ch=OUTPUT_CH,\n        skips=SKIPS,\n        input_ch_views=INPUT_CH_VIEWS,\n        use_viewdirs=USE_VIEWDIRS,\n    )\n\n    if return_metadata:\n        return model, INPUT_CH, INPUT_CH_VIEWS, OUTPUT_CH, SKIPS\n    else:\n        return model"
  },
  {
    "objectID": "reports/00_Setup_GenNerf.html#reproducing-vanilla-nerf-architecture",
    "href": "reports/00_Setup_GenNerf.html#reproducing-vanilla-nerf-architecture",
    "title": "Exploring the Vanilla NeRF Field + MLP",
    "section": "4.2 Reproducing Vanilla NeRF architecture",
    "text": "4.2 Reproducing Vanilla NeRF architecture\n\n4.2.1 Expected initial blockspecs\nThe following BlockSpecs should theoretically result in the same model as the vanilla NeRF model.\n\ninitial_blockspecs_to_reproduce_vanilla = BlockSpecs(\n    [\n        BlockSpec(256, 8, False, False),\n        BlockSpec(256, 8, False, False),\n        BlockSpec(256, 8, False, False),\n    ]\n)\n\nThe resulting architecture with the above BlockSpecs is shown below:\n\n\n\n\n\n\nTip\n\n\n\nModuleList (pts_linear) is the module corresponding to the MLP Base which we’re optimizing. Also while the class is named W23NeRF, it’s more apt to think of it as the Field component of the Vanilla NeRF model, and not the complete NeRF model itself.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe W23NeRF model is identical to the nerf-pytorch implementation of the vanilla NeRF model. However, this means it suffers from the same potential bug as the nerf-pytorch implementation, where the skip connection is applied at the 6th layer, instead of the 5th layer.\n\n\n\n\nDetailed architecture summary of W23NeRF model.\nmodel_w23, input_ch, input_ch_views, *rest = make_model_w23(\n    initial_blockspecs_to_reproduce_vanilla, return_metadata=True\n)\nsummary(model_w23, input_size=(input_ch + input_ch_views,))\n\n\n========================================================================================================================\nLayer (type (var_name):depth-idx)        Input Shape      Output Shape     Param #          Param %          Mult-Adds\n========================================================================================================================\nW23NeRF (W23NeRF)                        [90]             [4]              --                    --          --\n├─ModuleList (pts_linears): 1-1          --               --               --                    --          --\n│    └─Linear (0): 2-1                   [63]             [256]            16,384             2.75%          4,194,304\n│    └─Linear (1): 2-2                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (2): 2-3                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (3): 2-4                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (4): 2-5                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (5): 2-6                   [319]            [256]            81,920            13.75%          20,971,520\n│    └─Linear (6): 2-7                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (7): 2-8                   [256]            [256]            65,792            11.04%          16,842,752\n├─Linear (alpha_linear): 1-2             [256]            [1]              257                0.04%          257\n├─Linear (feature_linear): 1-3           [256]            [256]            65,792            11.04%          16,842,752\n├─ModuleList (views_linears): 1-4        --               --               --                    --          --\n│    └─Linear (0): 2-9                   [283]            [128]            36,352             6.10%          4,653,056\n├─Linear (rgb_linear): 1-5               [128]            [3]              387                0.06%          1,161\n========================================================================================================================\nTotal params: 595,844\nTrainable params: 595,844\nNon-trainable params: 0\nTotal mult-adds (M): 147.72\n========================================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.02\nParams size (MB): 2.38\nEstimated Total Size (MB): 2.40\n========================================================================================================================\n\n\n\n\n4.2.2 Initial BlockSpecs with large initial depths\nNow as an exercise, let’s try modifying the BlockSpecs input so that the initial depths for the first two blocks are arbitrarily large numbers. However, note that\n\n\n\n\n\n\nPotential Bug?\n\n\n\nThe resulting architecture is identical to the previous one, despite the new large depths specified for block 2 and 3. This indicates that the W23NeRF model does not actually take into account the depths for the first two blocks (this can be verified by looking at the implementation of W23NeRF in Section 4).\n\n\n\ninitial_blockspecs_test_large = BlockSpecs(\n    [\n        BlockSpec(256, 100000, False, False),\n        BlockSpec(256, 23904890238094283, False, False),\n        BlockSpec(256, 8, False, False),\n    ]\n)\n\n\n\nDetailed architecture summary of W23NeRF model.\nmodel_w23, input_ch, input_ch_views, *rest = make_model_w23(\n    initial_blockspecs_test_large, return_metadata=True\n)\nsummary(model_w23, input_size=(input_ch + input_ch_views,))\n\n\n========================================================================================================================\nLayer (type (var_name):depth-idx)        Input Shape      Output Shape     Param #          Param %          Mult-Adds\n========================================================================================================================\nW23NeRF (W23NeRF)                        [90]             [4]              --                    --          --\n├─ModuleList (pts_linears): 1-1          --               --               --                    --          --\n│    └─Linear (0): 2-1                   [63]             [256]            16,384             2.75%          4,194,304\n│    └─Linear (1): 2-2                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (2): 2-3                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (3): 2-4                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (4): 2-5                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (5): 2-6                   [319]            [256]            81,920            13.75%          20,971,520\n│    └─Linear (6): 2-7                   [256]            [256]            65,792            11.04%          16,842,752\n│    └─Linear (7): 2-8                   [256]            [256]            65,792            11.04%          16,842,752\n├─Linear (alpha_linear): 1-2             [256]            [1]              257                0.04%          257\n├─Linear (feature_linear): 1-3           [256]            [256]            65,792            11.04%          16,842,752\n├─ModuleList (views_linears): 1-4        --               --               --                    --          --\n│    └─Linear (0): 2-9                   [283]            [128]            36,352             6.10%          4,653,056\n├─Linear (rgb_linear): 1-5               [128]            [3]              387                0.06%          1,161\n========================================================================================================================\nTotal params: 595,844\nTrainable params: 595,844\nNon-trainable params: 0\nTotal mult-adds (M): 147.72\n========================================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.02\nParams size (MB): 2.38\nEstimated Total Size (MB): 2.40\n========================================================================================================================\n\n\n\n\n4.2.3 Initial blockspecs with small initial depth for block 3\nHowever, if we modify the initial depth for block 3, we see that the resulting architecture is different. This indicates that the W23NeRF model does take into account the depth for the third block.\n\ninitial_blockspecs_test_small_block3_depth = BlockSpecs(\n    [\n        BlockSpec(256, 8, False, False),\n        BlockSpec(256, 8, False, False),\n        BlockSpec(256, 4, False, False),\n    ]\n)\n\n\n\nDetailed architecture summary of W23NeRF model.\nmodel_w23, input_ch, input_ch_views, *rest = make_model_w23(\n    initial_blockspecs_test_small_block3_depth, return_metadata=True\n)\nsummary(model_w23, input_size=(input_ch + input_ch_views,))\n\n\n========================================================================================================================\nLayer (type (var_name):depth-idx)        Input Shape      Output Shape     Param #          Param %          Mult-Adds\n========================================================================================================================\nW23NeRF (W23NeRF)                        [90]             [4]              --                    --          --\n├─ModuleList (pts_linears): 1-1          --               --               --                    --          --\n│    └─Linear (0): 2-1                   [63]             [256]            16,384             4.92%          4,194,304\n│    └─Linear (1): 2-2                   [256]            [256]            65,792            19.78%          16,842,752\n│    └─Linear (2): 2-3                   [256]            [256]            65,792            19.78%          16,842,752\n│    └─Linear (3): 2-4                   [319]            [256]            81,920            24.62%          20,971,520\n├─Linear (alpha_linear): 1-2             [256]            [1]              257                0.08%          257\n├─Linear (feature_linear): 1-3           [256]            [256]            65,792            19.78%          16,842,752\n├─ModuleList (views_linears): 1-4        --               --               --                    --          --\n│    └─Linear (0): 2-5                   [283]            [128]            36,352            10.93%          4,653,056\n├─Linear (rgb_linear): 1-5               [128]            [3]              387                0.12%          1,161\n========================================================================================================================\nTotal params: 332,676\nTrainable params: 332,676\nNon-trainable params: 0\nTotal mult-adds (M): 80.35\n========================================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 1.33\nEstimated Total Size (MB): 1.34\n========================================================================================================================"
  },
  {
    "objectID": "reports/00_Setup_GenNerf.html#coarse-field-base-mlp-architecture-2",
    "href": "reports/00_Setup_GenNerf.html#coarse-field-base-mlp-architecture-2",
    "title": "Exploring the Vanilla NeRF Field + MLP",
    "section": "5.1 Coarse Field Base MLP Architecture",
    "text": "5.1 Coarse Field Base MLP Architecture\n\n\nDetailed architecture summary for GenNerf coarse field base MLP\nsummary(gen_nerf.field_coarse.mlp_base, input_size=(63,))\n\n\n========================================================================================================================\nLayer (type (var_name):depth-idx)        Input Shape      Output Shape     Param #          Param %          Mult-Adds\n========================================================================================================================\nCompressibleMLP (CompressibleMLP)        [63]             [256]            --                    --          --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (0): 2-1                   [63]             [256]            16,384             3.32%          4,194,304\n├─ReLU (activation): 1-2                 [256]            [256]            --                    --          --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (1): 2-2                   [256]            [256]            65,792            13.34%          16,842,752\n├─ReLU (activation): 1-4                 [256]            [256]            --               (recursive)      --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (2): 2-3                   [256]            [256]            65,792            13.34%          16,842,752\n├─ReLU (activation): 1-6                 [256]            [256]            --               (recursive)      --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (3): 2-4                   [256]            [256]            65,792            13.34%          16,842,752\n├─ReLU (activation): 1-8                 [256]            [256]            --               (recursive)      --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (4): 2-5                   [319]            [256]            81,920            16.61%          20,971,520\n├─ReLU (activation): 1-10                [256]            [256]            --               (recursive)      --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (5): 2-6                   [256]            [256]            65,792            13.34%          16,842,752\n├─ReLU (activation): 1-12                [256]            [256]            --               (recursive)      --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (6): 2-7                   [256]            [256]            65,792            13.34%          16,842,752\n├─ReLU (activation): 1-14                [256]            [256]            --               (recursive)      --\n├─ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --\n│    └─Linear (7): 2-8                   [256]            [256]            65,792            13.34%          16,842,752\n├─ReLU (out_activation): 1-16            [256]            [256]            --                    --          --\n========================================================================================================================\nTotal params: 493,056\nTrainable params: 493,056\nNon-trainable params: 0\nTotal mult-adds (M): 126.22\n========================================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.02\nParams size (MB): 1.97\nEstimated Total Size (MB): 1.99\n========================================================================================================================"
  },
  {
    "objectID": "reports/00_Setup_GenNerf.html#reproducing-vanilla-nerf-architecture-1",
    "href": "reports/00_Setup_GenNerf.html#reproducing-vanilla-nerf-architecture-1",
    "title": "Exploring the Vanilla NeRF Field + MLP",
    "section": "5.2 Reproducing Vanilla NeRF architecture",
    "text": "5.2 Reproducing Vanilla NeRF architecture\nLet’s validate that the default GenNerf model base MLP is identical to the VanillaNeRF model.\n\n\nTest case demonstrating that default GenNerf matches VanillaNerf implementation for the base MLP\nvanilla_pipeline_cfg = VanillaPipelineConfig()\nvanilla_nerf_cfg = VanillaModelConfig()\ndummy_datamanager = vanilla_pipeline_cfg.datamanager\nscene_box = SceneBox(\n    aabb=torch.tensor([[-1.5, -1.5, -1.5], [1.5, 1.5, 1.5]], dtype=torch.float32)\n)\nvanilla_nerf = vanilla_nerf_cfg.setup(\n    scene_box=scene_box,\n    num_train_data=100,\n).to(device=\"cpu\")\n\ngen_nerf.to(device=\"cpu\")\ntest_module_lists_equal(\n    gen_nerf.field_coarse.mlp_base.layers, vanilla_nerf.field_coarse.mlp_base.layers\n)\n\ninput_shape = [63]\nbaseline_flops = get_flops(vanilla_nerf.field_coarse.mlp_base, input_shape)\nbaseline_params = get_params(vanilla_nerf.field_coarse.mlp_base)\n\ngen_nerf_base_mlp_flops = get_flops(gen_nerf.field_coarse.mlp_base, input_shape)\ngen_nerf_base_mlp_params = get_params(gen_nerf.field_coarse.mlp_base)\n\n# Validate that GenNerf MLP Base matches VanillaNerf MLP Base flops and params\nassert (\n    baseline_flops == gen_nerf_base_mlp_flops\n), f\"GenNerf MLP Base flops {gen_nerf_base_mlp_flops} != VanillaNerf MLP Base flops {baseline_flops}\"\n\nassert (\n    baseline_params == gen_nerf_base_mlp_params\n), f\"GenNerf MLP Base params {gen_nerf_base_mlp_params} != VanillaNerf MLP Base params {baseline_params}\"\n\nprint(f\"GenNerf MLP Base matches VanillaNerf MLP Base!\")\n\n\nGenNerf MLP Base matches VanillaNerf MLP Base!\n\n\n\nget_flops(\n    vanilla_nerf.field_coarse.mlp_base,\n    input_shape,\n    verbose=True,\n    ret_layer_info=True,\n    report_missing=True,\n)\n\n[INFO] Register zero_ops() for &lt;class 'torch.nn.modules.activation.ReLU'&gt;.\n[INFO] Register count_linear() for &lt;class 'torch.nn.modules.linear.Linear'&gt;.\n[WARN] Cannot find rule for &lt;class 'torch.nn.modules.container.ModuleList'&gt;. Treat it as zero Macs and zero Params.\n[WARN] Cannot find rule for &lt;class 'nerfstudio.field_components.mlp.MLP'&gt;. Treat it as zero Macs and zero Params.\n\n\n(491008.0,\n 493056.0,\n {'activation': (0.0, 0.0, {}),\n  'out_activation': (0.0, 0.0, {}),\n  'layers': (491008.0,\n   493056.0,\n   {'0': (16128.0, 16384.0, {}),\n    '1': (65536.0, 65792.0, {}),\n    '2': (65536.0, 65792.0, {}),\n    '3': (65536.0, 65792.0, {}),\n    '4': (81664.0, 81920.0, {}),\n    '5': (65536.0, 65792.0, {}),\n    '6': (65536.0, 65792.0, {}),\n    '7': (65536.0, 65792.0, {})})})\n\n\n\nprint(*vanilla_nerf.field_coarse.mlp_base.named_children())\n\nfor n, m in vanilla_nerf.field_coarse.mlp_base.named_children():\n    if n == \"layers\":\n        print(f\"n: {m.total_ops.item()}, {m.total_params.item()}\")\n\n('activation', ReLU()) ('out_activation', ReLU()) ('layers', ModuleList(\n  (0): Linear(in_features=63, out_features=256, bias=True)\n  (1): Linear(in_features=256, out_features=256, bias=True)\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): Linear(in_features=256, out_features=256, bias=True)\n  (4): Linear(in_features=319, out_features=256, bias=True)\n  (5): Linear(in_features=256, out_features=256, bias=True)\n  (6): Linear(in_features=256, out_features=256, bias=True)\n  (7): Linear(in_features=256, out_features=256, bias=True)\n))\nn: 0.0, 0.0\n\n\n\nprint(*gen_nerf.field_coarse.mlp_base.named_children())\n\n('activation', ReLU()) ('out_activation', ReLU()) ('layers', ModuleList(\n  (0): Linear(in_features=63, out_features=256, bias=True)\n  (1): Linear(in_features=256, out_features=256, bias=True)\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): Linear(in_features=256, out_features=256, bias=True)\n  (4): Linear(in_features=319, out_features=256, bias=True)\n  (5): Linear(in_features=256, out_features=256, bias=True)\n  (6): Linear(in_features=256, out_features=256, bias=True)\n  (7): Linear(in_features=256, out_features=256, bias=True)\n))\n\n\n\nget_flops(\n    gen_nerf.field_coarse.mlp_base,\n    input_shape,\n    verbose=True,\n    ret_layer_info=True,\n    report_missing=True,\n)\n\n[INFO] Register zero_ops() for &lt;class 'torch.nn.modules.activation.ReLU'&gt;.\n[INFO] Register count_linear() for &lt;class 'torch.nn.modules.linear.Linear'&gt;.\n[WARN] Cannot find rule for &lt;class 'torch.nn.modules.container.ModuleList'&gt;. Treat it as zero Macs and zero Params.\n[WARN] Cannot find rule for &lt;class 'gen_nerf.modules.mlp.CompressibleMLP'&gt;. Treat it as zero Macs and zero Params.\n\n\n(491008.0,\n 493056.0,\n {'activation': (0.0, 0.0, {}),\n  'out_activation': (0.0, 0.0, {}),\n  'layers': (491008.0,\n   493056.0,\n   {'0': (16128.0, 16384.0, {}),\n    '1': (65536.0, 65792.0, {}),\n    '2': (65536.0, 65792.0, {}),\n    '3': (65536.0, 65792.0, {}),\n    '4': (81664.0, 81920.0, {}),\n    '5': (65536.0, 65792.0, {}),\n    '6': (65536.0, 65792.0, {}),\n    '7': (65536.0, 65792.0, {})})})\n\n\n\ncoarse_model, fine_model, grad_vars, input_ch, input_ch_views = create_nerf()\nget_flops(\n    coarse_model,\n    [63 + 27],\n    verbose=True,\n    ret_layer_info=True,\n    report_missing=True,\n)\n\n[INFO] Register count_linear() for &lt;class 'torch.nn.modules.linear.Linear'&gt;.\n[WARN] Cannot find rule for &lt;class 'torch.nn.modules.container.ModuleList'&gt;. Treat it as zero Macs and zero Params.\n[WARN] Cannot find rule for &lt;class '__main__.PyTorchNeRF'&gt;. Treat it as zero Macs and zero Params.\n\n\n(593408.0,\n 595844.0,\n {'pts_linears': (491008.0,\n   493056.0,\n   {'0': (16128.0, 16384.0, {}),\n    '1': (65536.0, 65792.0, {}),\n    '2': (65536.0, 65792.0, {}),\n    '3': (65536.0, 65792.0, {}),\n    '4': (65536.0, 65792.0, {}),\n    '5': (81664.0, 81920.0, {}),\n    '6': (65536.0, 65792.0, {}),\n    '7': (65536.0, 65792.0, {})}),\n  'views_linears': (36224.0, 36352.0, {'0': (36224.0, 36352.0, {})}),\n  'feature_linear': (65536.0, 65792.0, {}),\n  'alpha_linear': (256.0, 257.0, {}),\n  'rgb_linear': (384.0, 387.0, {})})\n\n\n\nsummary(gen_nerf.field_coarse)\n\n==================================================================\nLayer (type (var_name):depth-idx)                  Param #\n==================================================================\nGenNerfField (GenNerfField)                        --\n├─NeRFEncoding (position_encoding): 1-1            --\n├─NeRFEncoding (direction_encoding): 1-2           --\n├─CompressibleMLP (mlp_base): 1-3                  --\n│    └─ReLU (activation): 2-1                      --\n│    └─ReLU (out_activation): 2-2                  --\n│    └─ModuleList (layers): 2-3                    --\n│    │    └─Linear (0): 3-1                        16,384\n│    │    └─Linear (1): 3-2                        65,792\n│    │    └─Linear (2): 3-3                        65,792\n│    │    └─Linear (3): 3-4                        65,792\n│    │    └─Linear (4): 3-5                        81,920\n│    │    └─Linear (5): 3-6                        65,792\n│    │    └─Linear (6): 3-7                        65,792\n│    │    └─Linear (7): 3-8                        65,792\n├─MLP (mlp_head): 1-4                              --\n│    └─ReLU (activation): 2-4                      --\n│    └─ReLU (out_activation): 2-5                  --\n│    └─ModuleList (layers): 2-6                    --\n│    │    └─Linear (0): 3-9                        36,352\n│    │    └─Linear (1): 3-10                       16,512\n├─DensityFieldHead (field_output_density): 1-5     --\n│    └─Softplus (activation): 2-7                  --\n│    └─Linear (net): 2-8                           257\n├─ModuleList (field_heads): 1-6                    --\n│    └─RGBFieldHead (0): 2-9                       --\n│    │    └─Sigmoid (activation): 3-11             --\n│    │    └─Linear (net): 3-12                     387\n==================================================================\nTotal params: 546,564\nTrainable params: 546,564\nNon-trainable params: 0\n==================================================================\n\n\n\nsummary(vanilla_nerf.field_coarse)\n\n==================================================================\nLayer (type (var_name):depth-idx)                  Param #\n==================================================================\nNeRFField (NeRFField)                              --\n├─NeRFEncoding (position_encoding): 1-1            --\n├─NeRFEncoding (direction_encoding): 1-2           --\n├─MLP (mlp_base): 1-3                              --\n│    └─ReLU (activation): 2-1                      --\n│    └─ReLU (out_activation): 2-2                  --\n│    └─ModuleList (layers): 2-3                    --\n│    │    └─Linear (0): 3-1                        16,384\n│    │    └─Linear (1): 3-2                        65,792\n│    │    └─Linear (2): 3-3                        65,792\n│    │    └─Linear (3): 3-4                        65,792\n│    │    └─Linear (4): 3-5                        81,920\n│    │    └─Linear (5): 3-6                        65,792\n│    │    └─Linear (6): 3-7                        65,792\n│    │    └─Linear (7): 3-8                        65,792\n├─MLP (mlp_head): 1-4                              --\n│    └─ReLU (activation): 2-4                      --\n│    └─ReLU (out_activation): 2-5                  --\n│    └─ModuleList (layers): 2-6                    --\n│    │    └─Linear (0): 3-9                        36,352\n│    │    └─Linear (1): 3-10                       16,512\n├─DensityFieldHead (field_output_density): 1-5     --\n│    └─Softplus (activation): 2-7                  --\n│    └─Linear (net): 2-8                           257\n├─ModuleList (field_heads): 1-6                    --\n│    └─RGBFieldHead (0): 2-9                       --\n│    │    └─Sigmoid (activation): 3-11             --\n│    │    └─Linear (net): 3-12                     387\n==================================================================\nTotal params: 546,564\nTrainable params: 546,564\nNon-trainable params: 0\n=================================================================="
  },
  {
    "objectID": "reports/00_Setup_GenNerf.html#initial-blockspecs-to-generate-vanilla-nerf-architecture",
    "href": "reports/00_Setup_GenNerf.html#initial-blockspecs-to-generate-vanilla-nerf-architecture",
    "title": "Exploring the Vanilla NeRF Field + MLP",
    "section": "5.3 Initial BlockSpecs to generate Vanilla NeRF architecture",
    "text": "5.3 Initial BlockSpecs to generate Vanilla NeRF architecture\n\n\nBlockSpecs for creating an MLP Base for GenNerf that matches NerfStudio’s VanillaNerf implementation\nmlp_blocks_searcher: MLPBlocksSearcher = MLPBlocksSearcherConfig().setup(\n    input_shape=[63],\n    in_dim=63,\n    out_activation=nn.ReLU(),\n)\ninitial_blockspecs_to_reproduce_vanilla: MLPBlockSpecsConfig = (\n    mlp_blocks_searcher.get_blockspecs_config(BaseMLPArchStyle.VANILLA)\n)\nprint(initial_blockspecs_to_reproduce_vanilla)\n\nbase_mlp_to_reproduce_vanilla = mlp_blocks_searcher.generate_model(\n    initial_blockspecs_to_reproduce_vanilla.setup()\n)\n\n\nMLPBlockSpecsConfig:\n    _target: &lt;class 'gen_nerf.modules.mlp.MLPBlockSpecs'&gt;\n    stage1: BlockSpecConfig(_target=&lt;class 'darwinai.torch.builder.BlockSpec'&gt;, channels=256, depth=4, freeze_channel=False, freeze_depth=False)\n    stage2: BlockSpecConfig(_target=&lt;class 'darwinai.torch.builder.BlockSpec'&gt;, channels=256, depth=1, freeze_channel=False, freeze_depth=True)\n    stage3: BlockSpecConfig(_target=&lt;class 'darwinai.torch.builder.BlockSpec'&gt;, channels=256, depth=3, freeze_channel=False, freeze_depth=False)\n\n\n\n\nTest case demonstrating that the blockspecs registered to MLPBlocksSearcher for BaseMLPArchStyle.VANILLA match NerfStudio’s VanillaNerf MLP Base architecture\ntest_module_lists_equal(\n    base_mlp_to_reproduce_vanilla.layers, vanilla_nerf.field_coarse.mlp_base.layers\n)\nprint(f\"GenNerf MLP Base matches VanillaNerf MLP Base!\")\n\n\nGenNerf MLP Base matches VanillaNerf MLP Base!"
  },
  {
    "objectID": "reports/00_Setup_GenNerf.html#modifying-target-ratio-for-initial-blockspecs",
    "href": "reports/00_Setup_GenNerf.html#modifying-target-ratio-for-initial-blockspecs",
    "title": "Exploring the Vanilla NeRF Field + MLP",
    "section": "5.4 Modifying Target Ratio for Initial Blockspecs",
    "text": "5.4 Modifying Target Ratio for Initial Blockspecs\n\n5.4.1 Validating that target ratio of 1.0 results in identical architecture to VanillaNeRF Base MLP\n\n\nBlockSpecs for creating an MLP Base with target flops ratio of 1\nmlp_blocks_searcher_cfg = MLPBlocksSearcherConfig(\n    target_ratio=1.0, arch_style=BaseMLPArchStyle.LARGE_UNIFORM\n)\nmlp_blocks_searcher: MLPBlocksSearcher = mlp_blocks_searcher_cfg.setup(\n    input_shape=[63],\n    in_dim=63,\n    out_activation=nn.ReLU(),\n)\ngenerated_model = mlp_blocks_searcher.generated_model\ntest_module_lists_equal(\n    generated_model.layers, vanilla_nerf.field_coarse.mlp_base.layers\n)\n\n\nloggable_dict = mlp_blocks_searcher.loggable_dict\n# initial_blockspecs_to_reproduce_vanilla: MLPBlockSpecsConfig = (\n#     mlp_blocks_searcher.get_blockspecs_config(BaseMLPArchStyle.VANILLA)\n# )\n# print(initial_blockspecs_to_reproduce_vanilla)\n\n# base_mlp_to_reproduce_vanilla = mlp_blocks_searcher.generate_model(\n#     initial_blockspecs_to_reproduce_vanilla.setup()\n# )\n\n\n\nbaseline_flops = get_flops(vanilla_nerf.field_coarse.mlp_base, input_shape)\nbaseline_params = get_params(vanilla_nerf.field_coarse.mlp_base)\n\ngen_nerf_base_mlp_flops = get_flops(generated_model, input_shape)\ngen_nerf_base_mlp_params = get_params(generated_model)\n\n# Validate that GenNerf MLP Base matches VanillaNerf MLP Base flops and params\nassert (\n    baseline_flops == gen_nerf_base_mlp_flops\n), f\"GenNerf MLP Base flops {gen_nerf_base_mlp_flops} != VanillaNerf MLP Base flops {baseline_flops}\"\n\nassert (\n    baseline_params == gen_nerf_base_mlp_params\n), f\"GenNerf MLP Base params {gen_nerf_base_mlp_params} != VanillaNerf MLP Base params {baseline_params}\"\n\nprint(f\"GenNerf MLP Base matches VanillaNerf MLP Base!\")\n\nGenNerf MLP Base matches VanillaNerf MLP Base!\n\n\n\nloggable_dict\n\n{'generated': True,\n 'target_ratio': 1.0,\n 'build_metric': &lt;BuildMetrics.FLOPS: 1&gt;,\n 'pretrained': False,\n 'arch_style': &lt;BaseMLPArchStyle.LARGE_UNIFORM: 'LARGE_UNIFORM'&gt;,\n 'generated_mlp': {'params': 493056,\n  'flops': 491008,\n  'attributes': {'stage1': {'channels': 256,\n    'depth': 4,\n    'freeze_depth': False,\n    'freeze_channels': False},\n   'stage2': {'channels': 256,\n    'depth': 1,\n    'freeze_depth': True,\n    'freeze_channels': False},\n   'stage3': {'channels': 256,\n    'depth': 3,\n    'freeze_depth': False,\n    'freeze_channels': False},\n   'skip_connections': [4]}},\n 'baseline_vanilla_mlp': {'params': 493056,\n  'flops': 491008,\n  'attributes': {'stage1': {'channels': 256,\n    'depth': 4,\n    'freeze_depth': False,\n    'freeze_channels': False},\n   'stage2': {'channels': 256,\n    'depth': 1,\n    'freeze_depth': True,\n    'freeze_channels': False},\n   'stage3': {'channels': 256,\n    'depth': 3,\n    'freeze_depth': False,\n    'freeze_channels': False},\n   'skip_connections': [4]}},\n 'flop_ratio_vs_baseline_vanilla': 1.0,\n 'params_ratio_vs_baseline_vanilla': 1.0,\n 'baseline_initial_mlp': {'params': 493056,\n  'flops': 491008,\n  'attributes': {'stage1': {'channels': 256,\n    'depth': 4,\n    'freeze_depth': False,\n    'freeze_channels': False},\n   'stage2': {'channels': 256,\n    'depth': 1,\n    'freeze_depth': True,\n    'freeze_channels': False},\n   'stage3': {'channels': 256,\n    'depth': 3,\n    'freeze_depth': False,\n    'freeze_channels': False},\n   'skip_connections': [4]}},\n 'flop_ratio_vs_baseline_initial': 1.0,\n 'params_ratio_vs_baseline_initial': 1.0}\n\n\n\n\nclass MLPBlockSpecs(BlockSpecs):\n    \"\"\"A list of BlockSpecs for the MLPs in NeRF.\n    This class can be directly passed to `darwinai.torch.builder.build_model()` as\n    it satisfies the requirement of being a `Sequence[BlockSpec]`.\n\n    Args:\n        stage1: First stage of the base MLP in a NeRF Field.\n        stage2: Second stage of the base MLP in a NeRF Field.\n        stage3: Third stage of the base MLP in a NeRF Field.\n\n    Raises:\n        ValueError: If any of the BlockSpecs are None.\n    \"\"\"\n\n    def __init__(self, stage1: BlockSpec, stage2: BlockSpec, stage3: BlockSpec):\n        if stage1 is not None and stage2 is not None and stage3 is not None:\n            super().__init__([stage1, stage2, stage3])\n        else:\n            raise ValueError(\"BlockSpec for stage 1, 2, or 3 cannot be None.\")\n\n    @property\n    def stage1(self):\n        return self[0]\n\n    @property\n    def stage2(self):\n        return self[1]\n\n    @property\n    def stage3(self):\n        return self[2]\n\n\n\n\nclass BlockSpecs(UserList[BlockSpec]):\n    \"\"\"A pure wrapper around a list of BlockSpec objects.\n\n    This class satisfies the `Sequence[BlockSpec]` bound for `TBlockSpecs`,\n    so it can be passed to `darwinai.torch.builder.build_model()`. All architectures\n    that can be dynamically generated based on BlockSpec configurations should\n    subclass this class.\n\n    The constructor for this class is flexible. It can be passed a single sequence\n    of BlockSpec objects (such as a list or tuple), like this:\n\n        BlockSpecs([stage1, stage2, stage3])\n\n    Alternatively, it can be passed one or more BlockSpec objects directly, like this:\n\n        BlockSpecs(stage1, stage2, stage3)\n\n    In the latter case, the BlockSpec arguments are collected into a list in the\n    order they were passed.\n\n    Args:\n        blockspecs: One or more BlockSpec objects, or a single sequence of BlockSpec objects.\n    \"\"\"\n\n    def __init__(self, *blockspecs: Union[BlockSpec, Sequence[BlockSpec]]):\n        flat_blockspecs: List[BlockSpec] = []\n        for item in blockspecs:\n            if isinstance(item, Sequence):\n                flat_blockspecs.extend(item)\n            else:\n                flat_blockspecs.append(item)\n        super().__init__(flat_blockspecs)\n\n\n\n\nCode\nshow_doc(MLPBlockSpecs, title_level=3)\n\n\n\n\n5.4.2 MLPBlockSpecs\n\n MLPBlockSpecs (stage1:darwinai.torch.builder.BlockSpec,\n                stage2:darwinai.torch.builder.BlockSpec,\n                stage3:darwinai.torch.builder.BlockSpec)\n\nA list of BlockSpecs for the MLPs in NeRF. This class can be directly passed to darwinai.torch.builder.build_model() as it satisfies the requirement of being a Sequence[BlockSpec].\nArgs: stage1: First stage of the base MLP in a NeRF Field. stage2: Second stage of the base MLP in a NeRF Field. stage3: Third stage of the base MLP in a NeRF Field.\nRaises: ValueError: If any of the BlockSpecs are None.\n\n\n\n\nshow_doc(BlockSpecs, title_level=3)\n\n\n\n5.4.3 BlockSpecs\n\n BlockSpecs (*blockspecs:Union[darwinai.torch.builder.BlockSpec,Sequence[d\n             arwinai.torch.builder.BlockSpec]])\n\nA pure wrapper around a list of BlockSpec objects.\nThis class satisfies the Sequence[BlockSpec] bound for TBlockSpecs, so it can be passed to darwinai.torch.builder.build_model(). All architectures that can be dynamically generated based on BlockSpec configurations should subclass this class.\nThe constructor for this class is flexible. It can be passed a single sequence of BlockSpec objects (such as a list or tuple), like this:\nBlockSpecs([stage1, stage2, stage3])\nAlternatively, it can be passed one or more BlockSpec objects directly, like this:\nBlockSpecs(stage1, stage2, stage3)\nIn the latter case, the BlockSpec arguments are collected into a list in the order they were passed.\nArgs: blockspecs: One or more BlockSpec objects, or a single sequence of BlockSpec objects."
  },
  {
    "objectID": "reports/00_Setup_GenNerf.html#generate-the-blockspecs-produced-by-darwinai-sdk-for-the-w23nerf-model",
    "href": "reports/00_Setup_GenNerf.html#generate-the-blockspecs-produced-by-darwinai-sdk-for-the-w23nerf-model",
    "title": "Exploring the Vanilla NeRF Field + MLP",
    "section": "5.5 Generate the BlockSpecs produced by DarwinAI SDK for the W23NeRF model",
    "text": "5.5 Generate the BlockSpecs produced by DarwinAI SDK for the W23NeRF model\n\ndef get_darwin_params(make_model_fn, ratio, initial_blockspec, input_shape=[63 + 27]):\n    # Darwin Builder\n    target_flops_ratio = ratio\n\n    model = build_model(\n        make_model_fn,\n        initial_blockspec,\n        input_shape,\n        target_flops_ratio,\n        pretrained_model=None,\n    )\n\n    return model, model.Ws, model.D\n\n\ndef test_flop_ratios_w23(\n    initial_blockspec,\n    flop_ratios=[1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05],\n):\n    initial_blockspec_str = [(bs.channels, bs.depth) for bs in initial_blockspec]\n\n    input_shape = [63 + 27]\n\n    if len(set([bs.channels for bs in initial_blockspec])) == 1:\n        print(\n            f\"Initializing each stage to have the same layer width for model type W23NeRF.\"\n        )\n    else:\n        print(\n            f\"Initializing each stage to have different layer widths for model type W23NeRF\"\n        )\n\n    baseline_model = make_model_w23(initial_blockspec)\n    baseline_flops = get_flops(baseline_model, input_shape)\n    baseline_params = get_params(baseline_model)\n    print(\n        f\"Using initial blockspecs (List[BlockSpec(channel, depth)]): \\n\\t{initial_blockspec_str}.\"\n    )\n\n    print(f\"{'Target FR':&lt;10} {'Actual FR':&lt;10} {'Actual PR':&lt;10} {'Ws':&lt;20} {'D'}\")\n    for fr in flop_ratios:\n        model = build_model(\n            make_model_w23,\n            initial_blockspec,\n            input_shape,\n            fr,\n            pretrained_model=None,\n        )\n        new_flops = get_flops(model, input_shape)\n        new_params = get_params(model)\n        Ws_string = tuple(model.Ws[:3])\n\n        # Compute new flop ratio and params ratio.\n        flops_ratio = new_flops / baseline_flops\n        params_ratio = new_params / baseline_params\n\n        print(\n            f\"{fr:&lt;10.2f} {flops_ratio:&lt;10.2f} {params_ratio:&lt;10.2f} {str(Ws_string):&lt;20} {model.D}\"\n        )\n\n\n5.5.1 BlockSpecs generated by BuildSDK for the Eddy Vanilla NeRF with default=256\n\ndefault = 256\n\n# Same initial widths.\ninitial_blockspec = [\n    BlockSpec(default, 8, False, False),\n    BlockSpec(default, 8, False, False),\n    BlockSpec(default, 8, False, False),\n]\ntest_flop_ratios_w23(initial_blockspec)\n\nInitializing each stage to have the same layer width for model type W23NeRF.\nUsing initial blockspecs (List[BlockSpec(channel, depth)]): \n    [(256, 8), (256, 8), (256, 8)].\nTarget FR  Actual FR  Actual PR  Ws                   D\n1.00       1.00       1.00       (256, 256, 256)      8\n0.90       0.87       0.87       (253, 253, 253)      7\n0.80       0.82       0.82       (245, 245, 245)      7\n0.70       0.73       0.73       (231, 231, 231)      7\n0.60       0.60       0.60       (224, 224, 224)      6\n0.50       0.52       0.52       (208, 208, 208)      6\n0.40       0.38       0.38       (191, 191, 191)      5\n0.30       0.28       0.28       (177, 177, 177)      4\n0.20       0.20       0.20       (149, 149, 149)      4\n0.10       0.10       0.10       (102, 102, 102)      4\n0.05       0.05       0.05       (65, 65, 65)         4\n\n\n\n# Different initial widths.\ninitial_blockspec = [\n    BlockSpec(240, 8, False, False),\n    BlockSpec(248, 8, False, False),\n    BlockSpec(256, 8, False, False),\n]\ntest_flop_ratios(initial_blockspec, \"w23\")\n\nNameError: name 'test_flop_ratios' is not defined\n\n\n\n\n5.5.2 BlockSpecs generated by BuildSDK for the Eddy Vanilla NeRF with default=64\n\ndefault = 64\n\n# Same initial widths.\ninitial_blockspec = [\n    BlockSpec(default, 8, False, False),\n    BlockSpec(default, 8, False, False),\n    BlockSpec(default, 8, False, False),\n]\ntest_flop_ratios(initial_blockspec, \"w23\")\n\nInitializing each stage to have the same layer width for model type eddy.\nUsing initial blockspecs (List[BlockSpec(channel, depth)]): \n    [(64, 8), (64, 8), (64, 8)].\nTarget FR  Actual FR  Actual PR  Ws                   D\n1.00       1.00       1.00       (64, 64, 64)         8\n0.90       0.86       0.86       (62, 62, 62)         7\n0.80       0.76       0.76       (58, 58, 58)         7\n0.70       0.66       0.66       (57, 57, 57)         6\n0.60       0.59       0.59       (53, 53, 53)         6\n0.50       0.49       0.49       (51, 51, 51)         5\n0.40       0.40       0.40       (45, 45, 45)         5\n0.30       0.29       0.29       (40, 40, 40)         4\n0.20       0.21       0.21       (32, 32, 32)         4\n0.10       0.10       0.10       (19, 19, 19)         4\n0.05       0.06       0.06       (12, 12, 12)         5\n\n\n\n# Different initial widths.\ninitial_blockspec = [\n    BlockSpec(56, 4, False, True),\n    BlockSpec(60, 4, False, True),\n    BlockSpec(64, 6, False, False),\n]\ntest_flop_ratios(initial_blockspec, \"w23\")\n\nInitializing each stage to have different layer widths for model type eddy\nUsing initial blockspecs (List[BlockSpec(channel, depth)]): \n    [(56, 4), (60, 4), (64, 6)].\nTarget FR  Actual FR  Actual PR  Ws                   D\n1.00       1.00       1.00       (56, 60, 64)         6\n0.90       0.88       0.88       (55, 59, 63)         5\n0.80       0.77       0.77       (51, 55, 58)         5\n0.70       0.71       0.71       (48, 52, 56)         5\n0.60       0.59       0.59       (47, 51, 54)         4\n0.50       0.52       0.52       (44, 47, 50)         4\n0.40       0.38       0.38       (38, 41, 45)         3\n0.30       0.31       0.31       (33, 36, 39)         3\n0.20       0.20       0.20       (24, 26, 29)         3\n0.10       0.10       0.10       (14, 16, 18)         3\n0.05       0.06       0.06       (9, 11, 12)          3\n\n\n\n\n5.5.3 BlockSpecs generated by BuildSDK for the Vanilla GenNeRF with default=256\n\n# Same initial widths.\nstage1 = BlockSpec(256, 4, False, False)\nstage2 = BlockSpec(256, 1, False, False)\nstage3 = BlockSpec(256, 3, False, False)\n\ninitial_blockspec = GenNerfBaseMLPBlocks(stage1, stage2, stage3)\ntest_flop_ratios(initial_blockspec, \"gen_nerf\")\n\nInitializing each stage to have the same layer width for model type gen_nerf.\nUsing initial blockspecs (List[BlockSpec(channel, depth)]): \n    [(256, 4), (256, 1), (256, 3)].\nTarget FR  Actual FR  Actual PR  Ws                   D\n1.00       1.00       1.00       (256, 256, 256)      [4, 1, 3]\n0.90       0.73       0.73       (255, 255, 255)      [3, 1, 2]\n0.80       0.73       0.73       (255, 255, 255)      [3, 1, 2]\n0.70       0.67       0.67       (245, 245, 245)      [3, 1, 2]\n0.60       0.62       0.62       (235, 235, 235)      [3, 1, 2]\n0.50       0.52       0.52       (214, 214, 214)      [3, 1, 2]\n0.40       0.39       0.39       (203, 203, 203)      [2, 1, 2]\n0.30       0.33       0.33       (186, 186, 186)      [2, 1, 2]\n0.20       0.20       0.20       (162, 162, 162)      [2, 1, 1]\n0.10       0.10       0.10       (109, 109, 109)      [2, 1, 1]\n0.05       0.05       0.05       (74, 74, 74)         [2, 1, 1]\n\n\n\n# Different initial widths.\nstage1 = BlockSpec(240, 4, False, False)\nstage2 = BlockSpec(248, 1, False, False)\nstage3 = BlockSpec(256, 3, False, False)\n\ninitial_blockspec = GenNerfBaseMLPBlocks(stage1, stage2, stage3)\ntest_flop_ratios(initial_blockspec, \"gen_nerf\")\n\nInitializing each stage to have different layer widths for model type gen_nerf\nUsing initial blockspecs (List[BlockSpec(channel, depth)]): \n    [(240, 4), (248, 1), (256, 3)].\nTarget FR  Actual FR  Actual PR  Ws                   D\n1.00       1.00       1.00       (240, 248, 256)      [4, 1, 3]\n0.90       0.73       0.73       (239, 247, 255)      [3, 1, 2]\n0.80       0.73       0.73       (239, 247, 255)      [3, 1, 2]\n0.70       0.68       0.68       (230, 238, 245)      [3, 1, 2]\n0.60       0.62       0.62       (220, 227, 235)      [3, 1, 2]\n0.50       0.52       0.52       (200, 207, 214)      [3, 1, 2]\n0.40       0.39       0.40       (189, 196, 203)      [2, 1, 2]\n0.30       0.34       0.34       (174, 180, 186)      [2, 1, 2]\n0.20       0.20       0.20       (150, 156, 162)      [2, 1, 1]\n0.10       0.10       0.10       (100, 104, 109)      [2, 1, 1]\n0.05       0.05       0.05       (66, 70, 74)         [2, 1, 1]\n\n\n\n# Same initial widths.\nstage1 = BlockSpec(240, 4, False, False)\nstage2 = BlockSpec(248, 1, False, True)\nstage3 = BlockSpec(256, 3, False, False)\n\ninitial_blockspec = GenNerfBaseMLPBlocks(stage1, stage2, stage3)\ntest_flop_ratios(initial_blockspec, \"gen_nerf\")\n\nInitializing each stage to have different layer widths for model type gen_nerf\nUsing initial blockspecs (List[BlockSpec(channel, depth)]): \n    [(240, 4), (248, 1), (256, 3)].\nTarget FR  Actual FR  Actual PR  Ws                   D\n1.00       1.00       1.00       (240, 248, 256)      [4, 1, 3]\n0.90       0.73       0.73       (239, 247, 255)      [3, 1, 2]\n0.80       0.73       0.73       (239, 247, 255)      [3, 1, 2]\n0.70       0.68       0.68       (230, 238, 245)      [3, 1, 2]\n0.60       0.62       0.62       (220, 227, 235)      [3, 1, 2]\n0.50       0.52       0.52       (200, 207, 214)      [3, 1, 2]\n0.40       0.39       0.40       (189, 196, 203)      [2, 1, 2]\n0.30       0.34       0.34       (174, 180, 186)      [2, 1, 2]\n0.20       0.20       0.20       (150, 156, 162)      [2, 1, 1]\n0.10       0.10       0.10       (100, 104, 109)      [2, 1, 1]\n0.05       0.05       0.05       (66, 70, 74)         [2, 1, 1]"
  },
  {
    "objectID": "reports/index.html",
    "href": "reports/index.html",
    "title": "Reports",
    "section": "",
    "text": "Reports."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GEN-NeRF.",
    "section": "",
    "text": "Generating EfficieNt NeRFs through Neural Architecture Search"
  }
]