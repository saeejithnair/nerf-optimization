<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="And integrating this with DarwinAI SDKâ€™s darwinai.torch.builder.build_model()">

<title>GEN-NeRF - Exploring the Vanilla NeRF Field + MLP</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">GEN-NeRF</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../reports/index.html" rel="" target="" aria-current="page">
 <span class="menu-text">Reports</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../experiments/index.html" rel="" target="">
 <span class="menu-text">Experiments</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../documentation/index.html" rel="" target="">
 <span class="menu-text">Documentation</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../reports/index.html">Reports</a></li><li class="breadcrumb-item"><a href="../reports/00_Setup_GenNerf.html">Exploring the Vanilla NeRF Field + MLP</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../reports/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Reports</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../reports/00_Setup_GenNerf.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Exploring the Vanilla NeRF Field + MLP</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">1</span> Overview</a></li>
  <li><a href="#sec-nerf-pytorch" id="toc-sec-nerf-pytorch" class="nav-link" data-scroll-target="#sec-nerf-pytorch"><span class="header-section-number">2</span> Vanilla NeRF Model from <code>nerf-pytorch</code> repo</a>
  <ul class="collapse">
  <li><a href="#coarse-model-architecture" id="toc-coarse-model-architecture" class="nav-link" data-scroll-target="#coarse-model-architecture"><span class="header-section-number">2.1</span> Coarse Model Architecture</a></li>
  <li><a href="#coarse-field-base-mlp-architecture" id="toc-coarse-field-base-mlp-architecture" class="nav-link" data-scroll-target="#coarse-field-base-mlp-architecture"><span class="header-section-number">2.2</span> Coarse Field Base MLP Architecture</a></li>
  </ul></li>
  <li><a href="#sec-nerf-studio-vanilla" id="toc-sec-nerf-studio-vanilla" class="nav-link" data-scroll-target="#sec-nerf-studio-vanilla"><span class="header-section-number">3</span> Vanilla NeRF Model from NerfStudio</a>
  <ul class="collapse">
  <li><a href="#coarse-field-base-mlp-architecture-1" id="toc-coarse-field-base-mlp-architecture-1" class="nav-link" data-scroll-target="#coarse-field-base-mlp-architecture-1"><span class="header-section-number">3.1</span> Coarse Field Base MLP Architecture</a>
  <ul class="collapse">
  <li><a href="#discrepancy-in-the-base-mlp-architecture-between-nerf-pytorch-and-nerfstudio" id="toc-discrepancy-in-the-base-mlp-architecture-between-nerf-pytorch-and-nerfstudio" class="nav-link" data-scroll-target="#discrepancy-in-the-base-mlp-architecture-between-nerf-pytorch-and-nerfstudio"><span class="header-section-number">3.1.1</span> Discrepancy in the Base MLP Architecture between <code>nerf-pytorch</code> and <code>NerfStudio</code></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-w23-nerf" id="toc-sec-w23-nerf" class="nav-link" data-scroll-target="#sec-w23-nerf"><span class="header-section-number">4</span> Winter 2023 implementation of BlockSpec integrated vanilla NeRF model (<code>W23NeRF</code>)</a>
  <ul class="collapse">
  <li><a href="#creating-a-w23nerf-model" id="toc-creating-a-w23nerf-model" class="nav-link" data-scroll-target="#creating-a-w23nerf-model"><span class="header-section-number">4.1</span> Creating a <code>W23NeRF</code> model</a></li>
  <li><a href="#reproducing-vanilla-nerf-architecture" id="toc-reproducing-vanilla-nerf-architecture" class="nav-link" data-scroll-target="#reproducing-vanilla-nerf-architecture"><span class="header-section-number">4.2</span> Reproducing Vanilla NeRF architecture</a>
  <ul class="collapse">
  <li><a href="#expected-initial-blockspecs" id="toc-expected-initial-blockspecs" class="nav-link" data-scroll-target="#expected-initial-blockspecs"><span class="header-section-number">4.2.1</span> Expected initial blockspecs</a></li>
  <li><a href="#initial-blockspecs-with-large-initial-depths" id="toc-initial-blockspecs-with-large-initial-depths" class="nav-link" data-scroll-target="#initial-blockspecs-with-large-initial-depths"><span class="header-section-number">4.2.2</span> Initial BlockSpecs with large initial depths</a></li>
  <li><a href="#initial-blockspecs-with-small-initial-depth-for-block-3" id="toc-initial-blockspecs-with-small-initial-depth-for-block-3" class="nav-link" data-scroll-target="#initial-blockspecs-with-small-initial-depth-for-block-3"><span class="header-section-number">4.2.3</span> Initial blockspecs with small initial depth for block 3</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-gen-nerf" id="toc-sec-gen-nerf" class="nav-link" data-scroll-target="#sec-gen-nerf"><span class="header-section-number">5</span> Gen-NeRF Vanilla Model (<code>GenNerf</code>)</a>
  <ul class="collapse">
  <li><a href="#coarse-field-base-mlp-architecture-2" id="toc-coarse-field-base-mlp-architecture-2" class="nav-link" data-scroll-target="#coarse-field-base-mlp-architecture-2"><span class="header-section-number">5.1</span> Coarse Field Base MLP Architecture</a></li>
  <li><a href="#reproducing-vanilla-nerf-architecture-1" id="toc-reproducing-vanilla-nerf-architecture-1" class="nav-link" data-scroll-target="#reproducing-vanilla-nerf-architecture-1"><span class="header-section-number">5.2</span> Reproducing Vanilla NeRF architecture</a></li>
  <li><a href="#initial-blockspecs-to-generate-vanilla-nerf-architecture" id="toc-initial-blockspecs-to-generate-vanilla-nerf-architecture" class="nav-link" data-scroll-target="#initial-blockspecs-to-generate-vanilla-nerf-architecture"><span class="header-section-number">5.3</span> Initial BlockSpecs to generate Vanilla NeRF architecture</a></li>
  <li><a href="#modifying-target-ratio-for-initial-blockspecs" id="toc-modifying-target-ratio-for-initial-blockspecs" class="nav-link" data-scroll-target="#modifying-target-ratio-for-initial-blockspecs"><span class="header-section-number">5.4</span> Modifying Target Ratio for Initial Blockspecs</a>
  <ul class="collapse">
  <li><a href="#validating-that-target-ratio-of-1.0-results-in-identical-architecture-to-vanillanerf-base-mlp" id="toc-validating-that-target-ratio-of-1.0-results-in-identical-architecture-to-vanillanerf-base-mlp" class="nav-link" data-scroll-target="#validating-that-target-ratio-of-1.0-results-in-identical-architecture-to-vanillanerf-base-mlp"><span class="header-section-number">5.4.1</span> Validating that target ratio of 1.0 results in identical architecture to <code>VanillaNeRF</code> Base MLP</a></li>
  </ul></li>
  <li><a href="#generate-the-blockspecs-produced-by-darwinai-sdk-for-the-w23nerf-model" id="toc-generate-the-blockspecs-produced-by-darwinai-sdk-for-the-w23nerf-model" class="nav-link" data-scroll-target="#generate-the-blockspecs-produced-by-darwinai-sdk-for-the-w23nerf-model"><span class="header-section-number">5.5</span> Generate the BlockSpecs produced by DarwinAI SDK for the W23NeRF model</a>
  <ul class="collapse">
  <li><a href="#blockspecs-generated-by-buildsdk-for-the-eddy-vanilla-nerf-with-default256" id="toc-blockspecs-generated-by-buildsdk-for-the-eddy-vanilla-nerf-with-default256" class="nav-link" data-scroll-target="#blockspecs-generated-by-buildsdk-for-the-eddy-vanilla-nerf-with-default256"><span class="header-section-number">5.5.1</span> BlockSpecs generated by BuildSDK for the Eddy Vanilla NeRF with default=256</a></li>
  <li><a href="#blockspecs-generated-by-buildsdk-for-the-eddy-vanilla-nerf-with-default64" id="toc-blockspecs-generated-by-buildsdk-for-the-eddy-vanilla-nerf-with-default64" class="nav-link" data-scroll-target="#blockspecs-generated-by-buildsdk-for-the-eddy-vanilla-nerf-with-default64"><span class="header-section-number">5.5.2</span> BlockSpecs generated by BuildSDK for the Eddy Vanilla NeRF with default=64</a></li>
  <li><a href="#blockspecs-generated-by-buildsdk-for-the-vanilla-gennerf-with-default256" id="toc-blockspecs-generated-by-buildsdk-for-the-vanilla-gennerf-with-default256" class="nav-link" data-scroll-target="#blockspecs-generated-by-buildsdk-for-the-vanilla-gennerf-with-default256"><span class="header-section-number">5.5.3</span> BlockSpecs generated by BuildSDK for the Vanilla GenNeRF with default=256</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Exploring the Vanilla NeRF Field + MLP</h1>
</div>

<div>
  <div class="description">
    And integrating this with DarwinAI SDKâ€™s <code>darwinai.torch.builder.build_model()</code>
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="overview" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Overview</h1>
<p>This notebook does the following:</p>
<ol type="1">
<li><p>Loads and evaluates the vanilla NeRF model which Eddy modified (Wâ€™23) to integrate with the DarwinAI SDK</p></li>
<li><p>Loads and evaluates the new GenNerf model which weâ€™ve built leveraging NerfStudioâ€™s vanilla Nerf model</p></li>
<li><p>Shows how GenNerf can be integrated with the DarwinAI SDK</p></li>
</ol>
</section>
<section id="sec-nerf-pytorch" class="level1 page-columns page-full" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Vanilla NeRF Model from <a href="https://github.com/yenchenlin/nerf-pytorch"><code>nerf-pytorch</code></a> repo</h1>
<p><code>nerf-pytorch</code> is the defacto reproduction of the original NeRF paper, in PyTorch.</p>
<div id="lst-pytorch-nerf-implementation" class="cell" data-execution_count="6">
<details>
<summary>Core implementation of NeRF model from <code>pytorch-nerf</code> repo</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">class</span> PyTorchNeRF(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb1-3"><a href="#cb1-3"></a>        <span class="va">self</span>,</span>
<span id="cb1-4"><a href="#cb1-4"></a>        D<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb1-5"><a href="#cb1-5"></a>        W<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb1-6"><a href="#cb1-6"></a>        input_ch<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb1-7"><a href="#cb1-7"></a>        input_ch_views<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb1-8"><a href="#cb1-8"></a>        output_ch<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb1-9"><a href="#cb1-9"></a>        skips<span class="op">=</span>[<span class="dv">4</span>],</span>
<span id="cb1-10"><a href="#cb1-10"></a>        use_viewdirs<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-11"><a href="#cb1-11"></a>    ):</span>
<span id="cb1-12"><a href="#cb1-12"></a>        <span class="co">""" """</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>        <span class="bu">super</span>(PyTorchNeRF, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-14"><a href="#cb1-14"></a>        <span class="va">self</span>.D <span class="op">=</span> D</span>
<span id="cb1-15"><a href="#cb1-15"></a>        <span class="va">self</span>.W <span class="op">=</span> W</span>
<span id="cb1-16"><a href="#cb1-16"></a>        <span class="va">self</span>.input_ch <span class="op">=</span> input_ch</span>
<span id="cb1-17"><a href="#cb1-17"></a>        <span class="va">self</span>.input_ch_views <span class="op">=</span> input_ch_views</span>
<span id="cb1-18"><a href="#cb1-18"></a>        <span class="va">self</span>.skips <span class="op">=</span> skips</span>
<span id="cb1-19"><a href="#cb1-19"></a>        <span class="va">self</span>.use_viewdirs <span class="op">=</span> use_viewdirs</span>
<span id="cb1-20"><a href="#cb1-20"></a></span>
<span id="cb1-21"><a href="#cb1-21"></a>        <span class="va">self</span>.pts_linears <span class="op">=</span> nn.ModuleList(</span>
<span id="cb1-22"><a href="#cb1-22"></a>            [nn.Linear(input_ch, W)]</span>
<span id="cb1-23"><a href="#cb1-23"></a>            <span class="op">+</span> [</span>
<span id="cb1-24"><a href="#cb1-24"></a>                nn.Linear(W, W) <span class="cf">if</span> i <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.skips <span class="cf">else</span> nn.Linear(W <span class="op">+</span> input_ch, W)</span>
<span id="cb1-25"><a href="#cb1-25"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(D <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb1-26"><a href="#cb1-26"></a>            ]</span>
<span id="cb1-27"><a href="#cb1-27"></a>        )</span>
<span id="cb1-28"><a href="#cb1-28"></a></span>
<span id="cb1-29"><a href="#cb1-29"></a>        <span class="co">### Implementation according to the official code release (https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py#L104-L105)</span></span>
<span id="cb1-30"><a href="#cb1-30"></a>        <span class="va">self</span>.views_linears <span class="op">=</span> nn.ModuleList([nn.Linear(input_ch_views <span class="op">+</span> W, W <span class="op">//</span> <span class="dv">2</span>)])</span>
<span id="cb1-31"><a href="#cb1-31"></a></span>
<span id="cb1-32"><a href="#cb1-32"></a>        <span class="co">### Implementation according to the paper</span></span>
<span id="cb1-33"><a href="#cb1-33"></a>        <span class="co"># self.views_linears = nn.ModuleList(</span></span>
<span id="cb1-34"><a href="#cb1-34"></a>        <span class="co">#     [nn.Linear(input_ch_views + W, W//2)] + [nn.Linear(W//2, W//2) for i in range(D//2)])</span></span>
<span id="cb1-35"><a href="#cb1-35"></a></span>
<span id="cb1-36"><a href="#cb1-36"></a>        <span class="cf">if</span> use_viewdirs:</span>
<span id="cb1-37"><a href="#cb1-37"></a>            <span class="va">self</span>.feature_linear <span class="op">=</span> nn.Linear(W, W)</span>
<span id="cb1-38"><a href="#cb1-38"></a>            <span class="va">self</span>.alpha_linear <span class="op">=</span> nn.Linear(W, <span class="dv">1</span>)</span>
<span id="cb1-39"><a href="#cb1-39"></a>            <span class="va">self</span>.rgb_linear <span class="op">=</span> nn.Linear(W <span class="op">//</span> <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb1-40"><a href="#cb1-40"></a>        <span class="cf">else</span>:</span>
<span id="cb1-41"><a href="#cb1-41"></a>            <span class="va">self</span>.output_linear <span class="op">=</span> nn.Linear(W, output_ch)</span>
<span id="cb1-42"><a href="#cb1-42"></a></span>
<span id="cb1-43"><a href="#cb1-43"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-44"><a href="#cb1-44"></a>        input_pts, input_views <span class="op">=</span> torch.split(</span>
<span id="cb1-45"><a href="#cb1-45"></a>            x, [<span class="va">self</span>.input_ch, <span class="va">self</span>.input_ch_views], dim<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb1-46"><a href="#cb1-46"></a>        )</span>
<span id="cb1-47"><a href="#cb1-47"></a>        h <span class="op">=</span> input_pts</span>
<span id="cb1-48"><a href="#cb1-48"></a>        <span class="cf">for</span> i, l <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.pts_linears):</span>
<span id="cb1-49"><a href="#cb1-49"></a>            h <span class="op">=</span> <span class="va">self</span>.pts_linears[i](h)</span>
<span id="cb1-50"><a href="#cb1-50"></a>            h <span class="op">=</span> F.relu(h)</span>
<span id="cb1-51"><a href="#cb1-51"></a>            <span class="cf">if</span> i <span class="kw">in</span> <span class="va">self</span>.skips:</span>
<span id="cb1-52"><a href="#cb1-52"></a>                h <span class="op">=</span> torch.cat([input_pts, h], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-53"><a href="#cb1-53"></a></span>
<span id="cb1-54"><a href="#cb1-54"></a>        <span class="cf">if</span> <span class="va">self</span>.use_viewdirs:</span>
<span id="cb1-55"><a href="#cb1-55"></a>            alpha <span class="op">=</span> <span class="va">self</span>.alpha_linear(h)</span>
<span id="cb1-56"><a href="#cb1-56"></a>            feature <span class="op">=</span> <span class="va">self</span>.feature_linear(h)</span>
<span id="cb1-57"><a href="#cb1-57"></a>            h <span class="op">=</span> torch.cat([feature, input_views], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-58"><a href="#cb1-58"></a></span>
<span id="cb1-59"><a href="#cb1-59"></a>            <span class="cf">for</span> i, l <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.views_linears):</span>
<span id="cb1-60"><a href="#cb1-60"></a>                h <span class="op">=</span> <span class="va">self</span>.views_linears[i](h)</span>
<span id="cb1-61"><a href="#cb1-61"></a>                h <span class="op">=</span> F.relu(h)</span>
<span id="cb1-62"><a href="#cb1-62"></a></span>
<span id="cb1-63"><a href="#cb1-63"></a>            rgb <span class="op">=</span> <span class="va">self</span>.rgb_linear(h)</span>
<span id="cb1-64"><a href="#cb1-64"></a>            outputs <span class="op">=</span> torch.cat([rgb, alpha], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-65"><a href="#cb1-65"></a>        <span class="cf">else</span>:</span>
<span id="cb1-66"><a href="#cb1-66"></a>            outputs <span class="op">=</span> <span class="va">self</span>.output_linear(h)</span>
<span id="cb1-67"><a href="#cb1-67"></a></span>
<span id="cb1-68"><a href="#cb1-68"></a>        <span class="cf">return</span> outputs</span>
<span id="cb1-69"><a href="#cb1-69"></a></span>
<span id="cb1-70"><a href="#cb1-70"></a></span>
<span id="cb1-71"><a href="#cb1-71"></a><span class="kw">def</span> create_nerf():</span>
<span id="cb1-72"><a href="#cb1-72"></a>    INPUT_CH <span class="op">=</span> <span class="dv">63</span></span>
<span id="cb1-73"><a href="#cb1-73"></a>    OUTPUT_CH <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb1-74"><a href="#cb1-74"></a>    SKIPS <span class="op">=</span> [<span class="dv">4</span>]</span>
<span id="cb1-75"><a href="#cb1-75"></a>    INPUT_CH_VIEWS <span class="op">=</span> <span class="dv">27</span></span>
<span id="cb1-76"><a href="#cb1-76"></a>    USE_VIEWDIRS <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-77"><a href="#cb1-77"></a>    NETDEPTH <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb1-78"><a href="#cb1-78"></a>    NETWIDTH <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb1-79"><a href="#cb1-79"></a>    NETDEPTH_FINE <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb1-80"><a href="#cb1-80"></a>    NETWIDTH_FINE <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb1-81"><a href="#cb1-81"></a></span>
<span id="cb1-82"><a href="#cb1-82"></a>    model <span class="op">=</span> PyTorchNeRF(</span>
<span id="cb1-83"><a href="#cb1-83"></a>        D<span class="op">=</span>NETDEPTH,</span>
<span id="cb1-84"><a href="#cb1-84"></a>        W<span class="op">=</span>NETWIDTH,</span>
<span id="cb1-85"><a href="#cb1-85"></a>        input_ch<span class="op">=</span>INPUT_CH,</span>
<span id="cb1-86"><a href="#cb1-86"></a>        output_ch<span class="op">=</span>OUTPUT_CH,</span>
<span id="cb1-87"><a href="#cb1-87"></a>        skips<span class="op">=</span>SKIPS,</span>
<span id="cb1-88"><a href="#cb1-88"></a>        input_ch_views<span class="op">=</span>INPUT_CH_VIEWS,</span>
<span id="cb1-89"><a href="#cb1-89"></a>        use_viewdirs<span class="op">=</span>USE_VIEWDIRS,</span>
<span id="cb1-90"><a href="#cb1-90"></a>    )</span>
<span id="cb1-91"><a href="#cb1-91"></a>    grad_vars <span class="op">=</span> <span class="bu">list</span>(model.parameters())</span>
<span id="cb1-92"><a href="#cb1-92"></a></span>
<span id="cb1-93"><a href="#cb1-93"></a>    model_fine <span class="op">=</span> PyTorchNeRF(</span>
<span id="cb1-94"><a href="#cb1-94"></a>        D<span class="op">=</span>NETDEPTH_FINE,</span>
<span id="cb1-95"><a href="#cb1-95"></a>        W<span class="op">=</span>NETWIDTH_FINE,</span>
<span id="cb1-96"><a href="#cb1-96"></a>        input_ch<span class="op">=</span>INPUT_CH,</span>
<span id="cb1-97"><a href="#cb1-97"></a>        output_ch<span class="op">=</span>OUTPUT_CH,</span>
<span id="cb1-98"><a href="#cb1-98"></a>        skips<span class="op">=</span>SKIPS,</span>
<span id="cb1-99"><a href="#cb1-99"></a>        input_ch_views<span class="op">=</span>INPUT_CH_VIEWS,</span>
<span id="cb1-100"><a href="#cb1-100"></a>        use_viewdirs<span class="op">=</span>USE_VIEWDIRS,</span>
<span id="cb1-101"><a href="#cb1-101"></a>    )</span>
<span id="cb1-102"><a href="#cb1-102"></a>    grad_vars <span class="op">+=</span> <span class="bu">list</span>(model_fine.parameters())</span>
<span id="cb1-103"><a href="#cb1-103"></a></span>
<span id="cb1-104"><a href="#cb1-104"></a>    <span class="cf">return</span> model, model_fine, grad_vars, INPUT_CH, INPUT_CH_VIEWS</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Letâ€™s explore the architecture of the NeRF Field. Since the default configurations/architecture <strong>is identical</strong> for both the coarse and fine model, we will only explore the coarse field model.</p>
<section id="coarse-model-architecture" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="coarse-model-architecture"><span class="header-section-number">2.1</span> Coarse Model Architecture</h2>
<div class="cell" data-execution_count="7">
<details>
<summary>Architecture summary for <code>pytorch-nerf</code> coarse model</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>coarse_model, fine_model, grad_vars, input_ch, input_ch_views <span class="op">=</span> create_nerf()</span>
<span id="cb2-2"><a href="#cb2-2"></a>summary(coarse_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="pytorch-nerf-coarse-arch-summary" class="cell-output cell-output-display" data-execution_count="7">
<pre><code>========================================================
Layer (type (var_name):depth-idx)        Param #
========================================================
PyTorchNeRF (PyTorchNeRF)                --
â”œâ”€ModuleList (pts_linears): 1-1          --
â”‚    â””â”€Linear (0): 2-1                   16,384
â”‚    â””â”€Linear (1): 2-2                   65,792
â”‚    â””â”€Linear (2): 2-3                   65,792
â”‚    â””â”€Linear (3): 2-4                   65,792
â”‚    â””â”€Linear (4): 2-5                   65,792
â”‚    â””â”€Linear (5): 2-6                   81,920
â”‚    â””â”€Linear (6): 2-7                   65,792
â”‚    â””â”€Linear (7): 2-8                   65,792
â”œâ”€ModuleList (views_linears): 1-2        --
â”‚    â””â”€Linear (0): 2-9                   36,352
â”œâ”€Linear (feature_linear): 1-3           65,792
â”œâ”€Linear (alpha_linear): 1-4             257
â”œâ”€Linear (rgb_linear): 1-5               387
========================================================
Total params: 595,844
Trainable params: 595,844
Non-trainable params: 0
========================================================</code></pre>
</div>
</div>
</section>
<section id="coarse-field-base-mlp-architecture" class="level2 page-columns page-full" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="coarse-field-base-mlp-architecture"><span class="header-section-number">2.2</span> Coarse Field Base MLP Architecture</h2>
<p>Letâ€™s take a closer look at the model, especially the base MLP architecture (<code>pts_linears</code>), as itâ€™s what weâ€™ll be optimizing in GenNerf.</p>
<div class="cell page-columns page-full" data-execution_count="8">
<details>
<summary>Detailed architecture summary for <code>pytorch-nerf</code> coarse field base MLP</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>summary(coarse_model, input_size<span class="op">=</span>(input_ch <span class="op">+</span> input_ch_views,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="pytorch-nerf-coarse-base-mlp-summary" class="cell-output cell-output-display column-page-right" data-execution_count="8">
<pre><code>========================================================================================================================
Layer (type (var_name):depth-idx)        Input Shape      Output Shape     Param #          Param %          Mult-Adds
========================================================================================================================
PyTorchNeRF (PyTorchNeRF)                [90]             [4]              --                    --          --
â”œâ”€ModuleList (pts_linears): 1-1          --               --               --                    --          --
â”‚    â””â”€Linear (0): 2-1                   [63]             [256]            16,384             2.75%          4,194,304
â”‚    â””â”€Linear (1): 2-2                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (2): 2-3                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (3): 2-4                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (4): 2-5                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (5): 2-6                   [319]            [256]            81,920            13.75%          20,971,520
â”‚    â””â”€Linear (6): 2-7                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (7): 2-8                   [256]            [256]            65,792            11.04%          16,842,752
â”œâ”€Linear (alpha_linear): 1-2             [256]            [1]              257                0.04%          257
â”œâ”€Linear (feature_linear): 1-3           [256]            [256]            65,792            11.04%          16,842,752
â”œâ”€ModuleList (views_linears): 1-4        --               --               --                    --          --
â”‚    â””â”€Linear (0): 2-9                   [283]            [128]            36,352             6.10%          4,653,056
â”œâ”€Linear (rgb_linear): 1-5               [128]            [3]              387                0.06%          1,161
========================================================================================================================
Total params: 595,844
Trainable params: 595,844
Non-trainable params: 0
Total mult-adds (M): 147.72
========================================================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.02
Params size (MB): 2.38
Estimated Total Size (MB): 2.40
========================================================================================================================</code></pre>
</div>
</div>
</section>
</section>
<section id="sec-nerf-studio-vanilla" class="level1 page-columns page-full" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Vanilla NeRF Model from NerfStudio</h1>
<p>Since Gen-Nerf is built on top of NerfStudioâ€™s vanilla NeRF model, letâ€™s take a look at the vanilla NeRF model from NerfStudio. Unlike the previous implementation from <code>nerf-pytorch</code> in <a href="#sec-nerf-pytorch">Section&nbsp;2</a>, note that this model completely encapsulates NeRF, and not just the field.</p>
<p>Similar to before, note that the coarse and fine models are identical, so weâ€™ll only explore the coarse model.</p>
<div class="cell" data-execution_count="9">
<details>
<summary>Architecture summary for <code>VanillaModel</code> from NerfStudio</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>vanilla_pipeline_cfg <span class="op">=</span> VanillaPipelineConfig()</span>
<span id="cb6-2"><a href="#cb6-2"></a>vanilla_nerf_cfg <span class="op">=</span> VanillaModelConfig()</span>
<span id="cb6-3"><a href="#cb6-3"></a>scene_box <span class="op">=</span> SceneBox(</span>
<span id="cb6-4"><a href="#cb6-4"></a>    aabb<span class="op">=</span>torch.tensor([[<span class="op">-</span><span class="fl">1.5</span>, <span class="op">-</span><span class="fl">1.5</span>, <span class="op">-</span><span class="fl">1.5</span>], [<span class="fl">1.5</span>, <span class="fl">1.5</span>, <span class="fl">1.5</span>]], dtype<span class="op">=</span>torch.float32)</span>
<span id="cb6-5"><a href="#cb6-5"></a>)</span>
<span id="cb6-6"><a href="#cb6-6"></a>vanilla_nerf <span class="op">=</span> vanilla_nerf_cfg.setup(</span>
<span id="cb6-7"><a href="#cb6-7"></a>    scene_box<span class="op">=</span>scene_box,</span>
<span id="cb6-8"><a href="#cb6-8"></a>    num_train_data<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb6-9"><a href="#cb6-9"></a>)</span>
<span id="cb6-10"><a href="#cb6-10"></a>summary(vanilla_nerf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="nerfstudio-arch-summary" class="cell-output cell-output-display" data-execution_count="9">
<pre><code>============================================================================
Layer (type (var_name):depth-idx)                            Param #
============================================================================
NeRFModel (NeRFModel)                                        --
â”œâ”€NearFarCollider (collider): 1-1                            --
â”œâ”€NeRFField (field_coarse): 1-2                              --
â”‚    â””â”€NeRFEncoding (position_encoding): 2-1                 --
â”‚    â””â”€NeRFEncoding (direction_encoding): 2-2                --
â”‚    â””â”€MLP (mlp_base): 2-3                                   --
â”‚    â”‚    â””â”€ReLU (activation): 3-1                           --
â”‚    â”‚    â””â”€ReLU (out_activation): 3-2                       --
â”‚    â”‚    â””â”€ModuleList (layers): 3-3                         --
â”‚    â”‚    â”‚    â””â”€Linear (0): 4-1                             16,384
â”‚    â”‚    â”‚    â””â”€Linear (1): 4-2                             65,792
â”‚    â”‚    â”‚    â””â”€Linear (2): 4-3                             65,792
â”‚    â”‚    â”‚    â””â”€Linear (3): 4-4                             65,792
â”‚    â”‚    â”‚    â””â”€Linear (4): 4-5                             81,920
â”‚    â”‚    â”‚    â””â”€Linear (5): 4-6                             65,792
â”‚    â”‚    â”‚    â””â”€Linear (6): 4-7                             65,792
â”‚    â”‚    â”‚    â””â”€Linear (7): 4-8                             65,792
â”‚    â””â”€MLP (mlp_head): 2-4                                   --
â”‚    â”‚    â””â”€ReLU (activation): 3-4                           --
â”‚    â”‚    â””â”€ReLU (out_activation): 3-5                       --
â”‚    â”‚    â””â”€ModuleList (layers): 3-6                         --
â”‚    â”‚    â”‚    â””â”€Linear (0): 4-9                             36,352
â”‚    â”‚    â”‚    â””â”€Linear (1): 4-10                            16,512
â”‚    â””â”€DensityFieldHead (field_output_density): 2-5          --
â”‚    â”‚    â””â”€Softplus (activation): 3-7                       --
â”‚    â”‚    â””â”€Linear (net): 3-8                                257
â”‚    â””â”€ModuleList (field_heads): 2-6                         --
â”‚    â”‚    â””â”€RGBFieldHead (0): 3-9                            --
â”‚    â”‚    â”‚    â””â”€Sigmoid (activation): 4-11                  --
â”‚    â”‚    â”‚    â””â”€Linear (net): 4-12                          387
â”œâ”€NeRFField (field_fine): 1-3                                --
â”‚    â””â”€NeRFEncoding (position_encoding): 2-7                 --
â”‚    â””â”€NeRFEncoding (direction_encoding): 2-8                --
â”‚    â””â”€MLP (mlp_base): 2-9                                   --
â”‚    â”‚    â””â”€ReLU (activation): 3-10                          --
â”‚    â”‚    â””â”€ReLU (out_activation): 3-11                      --
â”‚    â”‚    â””â”€ModuleList (layers): 3-12                        --
â”‚    â”‚    â”‚    â””â”€Linear (0): 4-13                            16,384
â”‚    â”‚    â”‚    â””â”€Linear (1): 4-14                            65,792
â”‚    â”‚    â”‚    â””â”€Linear (2): 4-15                            65,792
â”‚    â”‚    â”‚    â””â”€Linear (3): 4-16                            65,792
â”‚    â”‚    â”‚    â””â”€Linear (4): 4-17                            81,920
â”‚    â”‚    â”‚    â””â”€Linear (5): 4-18                            65,792
â”‚    â”‚    â”‚    â””â”€Linear (6): 4-19                            65,792
â”‚    â”‚    â”‚    â””â”€Linear (7): 4-20                            65,792
â”‚    â””â”€MLP (mlp_head): 2-10                                  --
â”‚    â”‚    â””â”€ReLU (activation): 3-13                          --
â”‚    â”‚    â””â”€ReLU (out_activation): 3-14                      --
â”‚    â”‚    â””â”€ModuleList (layers): 3-15                        --
â”‚    â”‚    â”‚    â””â”€Linear (0): 4-21                            36,352
â”‚    â”‚    â”‚    â””â”€Linear (1): 4-22                            16,512
â”‚    â””â”€DensityFieldHead (field_output_density): 2-11         --
â”‚    â”‚    â””â”€Softplus (activation): 3-16                      --
â”‚    â”‚    â””â”€Linear (net): 3-17                               257
â”‚    â””â”€ModuleList (field_heads): 2-12                        387
â”‚    â”‚    â””â”€RGBFieldHead (0): 3-18                           (recursive)
â”‚    â”‚    â”‚    â””â”€Sigmoid (activation): 4-23                  --
â”‚    â”‚    â”‚    â””â”€Linear (net): 4-24                          (recursive)
â”œâ”€UniformSampler (sampler_uniform): 1-4                      --
â”œâ”€PDFSampler (sampler_pdf): 1-5                              --
â”œâ”€RGBRenderer (renderer_rgb): 1-6                            --
â”œâ”€AccumulationRenderer (renderer_accumulation): 1-7          --
â”œâ”€DepthRenderer (renderer_depth): 1-8                        --
â”œâ”€MSELoss (rgb_loss): 1-9                                    --
â”œâ”€PeakSignalNoiseRatio (psnr): 1-10                          --
â”œâ”€LearnedPerceptualImagePatchSimilarity (lpips): 1-11        --
â”‚    â””â”€NoTrainLpips (net): 2-13                              --
â”‚    â”‚    â””â”€ScalingLayer (scaling_layer): 3-19               --
â”‚    â”‚    â””â”€alexnet (net): 3-20                              --
â”‚    â”‚    â”‚    â””â”€Sequential (slice1): 4-25                   (23,296)
â”‚    â”‚    â”‚    â””â”€Sequential (slice2): 4-26                   (307,392)
â”‚    â”‚    â”‚    â””â”€Sequential (slice3): 4-27                   (663,936)
â”‚    â”‚    â”‚    â””â”€Sequential (slice4): 4-28                   (884,992)
â”‚    â”‚    â”‚    â””â”€Sequential (slice5): 4-29                   (590,080)
â”‚    â”‚    â””â”€NetLinLayer (lin0): 3-21                         --
â”‚    â”‚    â”‚    â””â”€Sequential (model): 4-30                    64
â”‚    â”‚    â””â”€NetLinLayer (lin1): 3-22                         --
â”‚    â”‚    â”‚    â””â”€Sequential (model): 4-31                    192
â”‚    â”‚    â””â”€NetLinLayer (lin2): 3-23                         --
â”‚    â”‚    â”‚    â””â”€Sequential (model): 4-32                    384
â”‚    â”‚    â””â”€NetLinLayer (lin3): 3-24                         --
â”‚    â”‚    â”‚    â””â”€Sequential (model): 4-33                    256
â”‚    â”‚    â””â”€NetLinLayer (lin4): 3-25                         --
â”‚    â”‚    â”‚    â””â”€Sequential (model): 4-34                    256
â”‚    â”‚    â””â”€ModuleList (lins): 3-26                          1,152
â”‚    â”‚    â”‚    â””â”€NetLinLayer (0): 4-35                       (recursive)
â”‚    â”‚    â”‚    â””â”€NetLinLayer (1): 4-36                       (recursive)
â”‚    â”‚    â”‚    â””â”€NetLinLayer (2): 4-37                       (recursive)
â”‚    â”‚    â”‚    â””â”€NetLinLayer (3): 4-38                       (recursive)
â”‚    â”‚    â”‚    â””â”€NetLinLayer (4): 4-39                       (recursive)
============================================================================
Total params: 3,565,128
Trainable params: 1,095,432
Non-trainable params: 2,469,696
============================================================================</code></pre>
</div>
</div>
<section id="coarse-field-base-mlp-architecture-1" class="level2 page-columns page-full" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="coarse-field-base-mlp-architecture-1"><span class="header-section-number">3.1</span> Coarse Field Base MLP Architecture</h2>
<div class="cell page-columns page-full" data-execution_count="10">
<details>
<summary>Detailed architecture summary for <code>VanillaModel</code> coarse field base MLP from NerfStudio</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>summary(vanilla_nerf.field_coarse.mlp_base, input_size<span class="op">=</span>(<span class="dv">63</span>,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="nerfstudio-coarse-base-mlp-summary" class="cell-output cell-output-display column-page-right" data-execution_count="10">
<pre><code>========================================================================================================================
Layer (type (var_name):depth-idx)        Input Shape      Output Shape     Param #          Param %          Mult-Adds
========================================================================================================================
MLP (MLP)                                [63]             [256]            --                    --          --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (0): 2-1                   [63]             [256]            16,384             3.32%          4,194,304
â”œâ”€ReLU (activation): 1-2                 [256]            [256]            --                    --          --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (1): 2-2                   [256]            [256]            65,792            13.34%          16,842,752
â”œâ”€ReLU (activation): 1-4                 [256]            [256]            --               (recursive)      --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (2): 2-3                   [256]            [256]            65,792            13.34%          16,842,752
â”œâ”€ReLU (activation): 1-6                 [256]            [256]            --               (recursive)      --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (3): 2-4                   [256]            [256]            65,792            13.34%          16,842,752
â”œâ”€ReLU (activation): 1-8                 [256]            [256]            --               (recursive)      --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (4): 2-5                   [319]            [256]            81,920            16.61%          20,971,520
â”œâ”€ReLU (activation): 1-10                [256]            [256]            --               (recursive)      --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (5): 2-6                   [256]            [256]            65,792            13.34%          16,842,752
â”œâ”€ReLU (activation): 1-12                [256]            [256]            --               (recursive)      --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (6): 2-7                   [256]            [256]            65,792            13.34%          16,842,752
â”œâ”€ReLU (activation): 1-14                [256]            [256]            --               (recursive)      --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (7): 2-8                   [256]            [256]            65,792            13.34%          16,842,752
â”œâ”€ReLU (out_activation): 1-16            [256]            [256]            --                    --          --
========================================================================================================================
Total params: 493,056
Trainable params: 493,056
Non-trainable params: 0
Total mult-adds (M): 126.22
========================================================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.02
Params size (MB): 1.97
Estimated Total Size (MB): 1.99
========================================================================================================================</code></pre>
</div>
</div>
<section id="discrepancy-in-the-base-mlp-architecture-between-nerf-pytorch-and-nerfstudio" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="discrepancy-in-the-base-mlp-architecture-between-nerf-pytorch-and-nerfstudio"><span class="header-section-number">3.1.1</span> Discrepancy in the Base MLP Architecture between <code>nerf-pytorch</code> and <code>NerfStudio</code></h3>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Potential Bug?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that thereâ€™s a discrepancy in the depth at which the skip connection is applied. In the <code>nerf-pytorch</code> implementation, the skip connection is applied at the 6th layer, whereas in NerfStudioâ€™s implementation, the skip connection is applied at the 5th layer.</p>
</div>
</div>
<div class="cell" data-execution_count="11">
<details>
<summary>Test case demonstrating discrepancy between <code>pytorch-nerf</code> and <code>nerfstudio</code> implementations</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>coarse_model, <span class="op">*</span>rest <span class="op">=</span> create_nerf()</span>
<span id="cb10-2"><a href="#cb10-2"></a></span>
<span id="cb10-3"><a href="#cb10-3"></a>vanilla_pipeline_cfg <span class="op">=</span> VanillaPipelineConfig()</span>
<span id="cb10-4"><a href="#cb10-4"></a>vanilla_nerf_cfg <span class="op">=</span> VanillaModelConfig()</span>
<span id="cb10-5"><a href="#cb10-5"></a>dummy_datamanager <span class="op">=</span> vanilla_pipeline_cfg.datamanager</span>
<span id="cb10-6"><a href="#cb10-6"></a>scene_box <span class="op">=</span> SceneBox(</span>
<span id="cb10-7"><a href="#cb10-7"></a>    aabb<span class="op">=</span>torch.tensor([[<span class="op">-</span><span class="fl">1.5</span>, <span class="op">-</span><span class="fl">1.5</span>, <span class="op">-</span><span class="fl">1.5</span>], [<span class="fl">1.5</span>, <span class="fl">1.5</span>, <span class="fl">1.5</span>]], dtype<span class="op">=</span>torch.float32)</span>
<span id="cb10-8"><a href="#cb10-8"></a>)</span>
<span id="cb10-9"><a href="#cb10-9"></a>vanilla_nerf <span class="op">=</span> vanilla_nerf_cfg.setup(</span>
<span id="cb10-10"><a href="#cb10-10"></a>    scene_box<span class="op">=</span>scene_box,</span>
<span id="cb10-11"><a href="#cb10-11"></a>    num_train_data<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb10-12"><a href="#cb10-12"></a>)</span>
<span id="cb10-13"><a href="#cb10-13"></a></span>
<span id="cb10-14"><a href="#cb10-14"></a>test_module_lists_equal(</span>
<span id="cb10-15"><a href="#cb10-15"></a>    coarse_model.pts_linears, vanilla_nerf.field_coarse.mlp_base.layers</span>
<span id="cb10-16"><a href="#cb10-16"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>Traceback (most recent call last):</span>
<span id="cb11-2"><a href="#cb11-2"></a>  File <span class="st">"/tmp/ipykernel_807250/1772845051.py"</span>, line <span class="dv">123</span>, <span class="kw">in</span> test_module_lists_equal</span>
<span id="cb11-3"><a href="#cb11-3"></a>    <span class="cf">assert</span> module1.in_features <span class="op">==</span> module2.in_features, (</span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="pp">AssertionError</span>: Module <span class="dv">4</span>: Linear in_features differ (<span class="dv">256</span> vs <span class="dv">319</span>)</span>
<span id="cb11-5"><a href="#cb11-5"></a></span>
<span id="cb11-6"><a href="#cb11-6"></a>List <span class="dv">1</span>: ModuleList(</span>
<span id="cb11-7"><a href="#cb11-7"></a>  (<span class="dv">0</span>): Linear(in_features<span class="op">=</span><span class="dv">63</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-8"><a href="#cb11-8"></a>  (<span class="dv">1</span>): Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-9"><a href="#cb11-9"></a>  (<span class="dv">2</span>): Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-10"><a href="#cb11-10"></a>  (<span class="dv">3</span>): Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-11"><a href="#cb11-11"></a>  (<span class="dv">4</span>): Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-12"><a href="#cb11-12"></a>  (<span class="dv">5</span>): Linear(in_features<span class="op">=</span><span class="dv">319</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-13"><a href="#cb11-13"></a>  (<span class="dv">6</span>): Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-14"><a href="#cb11-14"></a>  (<span class="dv">7</span>): Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-15"><a href="#cb11-15"></a>)</span>
<span id="cb11-16"><a href="#cb11-16"></a>List <span class="dv">2</span>: ModuleList(</span>
<span id="cb11-17"><a href="#cb11-17"></a>  (<span class="dv">0</span>): Linear(in_features<span class="op">=</span><span class="dv">63</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-18"><a href="#cb11-18"></a>  (<span class="dv">1</span>): Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-19"><a href="#cb11-19"></a>  (<span class="dv">2</span>): Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-20"><a href="#cb11-20"></a>  (<span class="dv">3</span>): Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-21"><a href="#cb11-21"></a>  (<span class="dv">4</span>): Linear(in_features<span class="op">=</span><span class="dv">319</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-22"><a href="#cb11-22"></a>  (<span class="dv">5</span>): Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-23"><a href="#cb11-23"></a>  (<span class="dv">6</span>): Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-24"><a href="#cb11-24"></a>  (<span class="dv">7</span>): Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-25"><a href="#cb11-25"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section>
</section>
</section>
<section id="sec-w23-nerf" class="level1 page-columns page-full" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Winter 2023 implementation of BlockSpec integrated vanilla NeRF model (<code>W23NeRF</code>)</h1>
<p>This model is used as a pseudo-baseline to identify any bugs/discrepancies in either this (Wâ€™23) implementation or the GenNerf implementation. Eddy performed a lot of experiments in Wâ€™23 which we donâ€™t necessarily want to throw out, but itâ€™s important to ensure that the results can be replicated, and if not, understand where exactly the limitations lie.</p>
<p>The model modifies the core implementation from <a href="#sec-nerf-pytorch">Section&nbsp;2</a>, and showed how it could be integrated with the DarwinAI SDK.</p>
<div id="lst-w23-nerf-implementation" class="cell" data-execution_count="12">
<details>
<summary>Core implementation of <code>W23NeRF</code> model (modified by Eddy to enable integration with DarwinAI SDK)</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="kw">class</span> W23NeRF(nn.Module):</span>
<span id="cb12-2"><a href="#cb12-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb12-3"><a href="#cb12-3"></a>        <span class="va">self</span>,</span>
<span id="cb12-4"><a href="#cb12-4"></a>        blockspec: Optional[List[BlockSpec]],</span>
<span id="cb12-5"><a href="#cb12-5"></a>        input_ch<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb12-6"><a href="#cb12-6"></a>        input_ch_views<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb12-7"><a href="#cb12-7"></a>        output_ch<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb12-8"><a href="#cb12-8"></a>        skips<span class="op">=</span>[<span class="dv">4</span>],</span>
<span id="cb12-9"><a href="#cb12-9"></a>        use_viewdirs<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb12-10"><a href="#cb12-10"></a>    ):</span>
<span id="cb12-11"><a href="#cb12-11"></a>        <span class="co">"""Vanilla NeRF implementation in PyTorch with modifications by Eddy."""</span></span>
<span id="cb12-12"><a href="#cb12-12"></a>        <span class="bu">super</span>(W23NeRF, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb12-13"><a href="#cb12-13"></a>        <span class="va">self</span>.blockspecs <span class="op">=</span> []</span>
<span id="cb12-14"><a href="#cb12-14"></a>        <span class="cf">if</span> blockspec <span class="kw">is</span> <span class="va">None</span>:  <span class="co"># should there be no blockspec, revert to default values</span></span>
<span id="cb12-15"><a href="#cb12-15"></a>            <span class="va">self</span>.blockspecs <span class="op">=</span> [BlockSpec(<span class="dv">256</span>, <span class="dv">8</span>), BlockSpec(<span class="dv">256</span>, <span class="dv">8</span>), BlockSpec(<span class="dv">256</span>, <span class="dv">8</span>)]</span>
<span id="cb12-16"><a href="#cb12-16"></a>        <span class="cf">else</span>:</span>
<span id="cb12-17"><a href="#cb12-17"></a>            <span class="va">self</span>.blockspecs <span class="op">=</span> blockspec</span>
<span id="cb12-18"><a href="#cb12-18"></a></span>
<span id="cb12-19"><a href="#cb12-19"></a>        <span class="va">self</span>.D <span class="op">=</span> <span class="va">self</span>.blockspecs[<span class="dv">2</span>].depth</span>
<span id="cb12-20"><a href="#cb12-20"></a>        <span class="va">self</span>.W <span class="op">=</span> <span class="va">self</span>.blockspecs[<span class="dv">2</span>].channels</span>
<span id="cb12-21"><a href="#cb12-21"></a>        <span class="va">self</span>.Ws <span class="op">=</span> []</span>
<span id="cb12-22"><a href="#cb12-22"></a></span>
<span id="cb12-23"><a href="#cb12-23"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb12-24"><a href="#cb12-24"></a>            <span class="va">self</span>.Ws.append(<span class="va">self</span>.blockspecs[i].channels)</span>
<span id="cb12-25"><a href="#cb12-25"></a></span>
<span id="cb12-26"><a href="#cb12-26"></a>        <span class="va">self</span>.input_ch <span class="op">=</span> input_ch</span>
<span id="cb12-27"><a href="#cb12-27"></a>        <span class="va">self</span>.input_ch_views <span class="op">=</span> input_ch_views</span>
<span id="cb12-28"><a href="#cb12-28"></a>        <span class="va">self</span>.skips <span class="op">=</span> []</span>
<span id="cb12-29"><a href="#cb12-29"></a>        <span class="va">self</span>.skips.append(<span class="va">self</span>.D <span class="op">//</span> <span class="dv">2</span>)</span>
<span id="cb12-30"><a href="#cb12-30"></a></span>
<span id="cb12-31"><a href="#cb12-31"></a>        <span class="cf">if</span> <span class="va">self</span>.skips[<span class="dv">0</span>] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb12-32"><a href="#cb12-32"></a>            <span class="cf">return</span></span>
<span id="cb12-33"><a href="#cb12-33"></a>        <span class="va">self</span>.Ws.append(<span class="va">self</span>.skips)</span>
<span id="cb12-34"><a href="#cb12-34"></a>        <span class="va">self</span>.use_viewdirs <span class="op">=</span> use_viewdirs</span>
<span id="cb12-35"><a href="#cb12-35"></a></span>
<span id="cb12-36"><a href="#cb12-36"></a>        <span class="va">self</span>.pts_linears <span class="op">=</span> nn.ModuleList([nn.Linear(input_ch, <span class="va">self</span>.Ws[<span class="dv">0</span>])])</span>
<span id="cb12-37"><a href="#cb12-37"></a>        switch <span class="op">=</span> <span class="va">False</span></span>
<span id="cb12-38"><a href="#cb12-38"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.D <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb12-39"><a href="#cb12-39"></a>            <span class="cf">if</span> i <span class="kw">in</span> <span class="va">self</span>.skips:</span>
<span id="cb12-40"><a href="#cb12-40"></a>                switch <span class="op">=</span> <span class="va">True</span></span>
<span id="cb12-41"><a href="#cb12-41"></a>                <span class="va">self</span>.pts_linears.append(nn.Linear(<span class="va">self</span>.Ws[<span class="dv">1</span>] <span class="op">+</span> input_ch, <span class="va">self</span>.Ws[<span class="dv">2</span>]))</span>
<span id="cb12-42"><a href="#cb12-42"></a>                <span class="cf">continue</span></span>
<span id="cb12-43"><a href="#cb12-43"></a></span>
<span id="cb12-44"><a href="#cb12-44"></a>            <span class="cf">if</span> <span class="kw">not</span> switch:</span>
<span id="cb12-45"><a href="#cb12-45"></a>                <span class="cf">if</span> i <span class="op">+</span> <span class="dv">1</span> <span class="kw">in</span> <span class="va">self</span>.skips:</span>
<span id="cb12-46"><a href="#cb12-46"></a>                    <span class="va">self</span>.pts_linears.append(nn.Linear(<span class="va">self</span>.Ws[<span class="dv">0</span>], <span class="va">self</span>.Ws[<span class="dv">1</span>]))</span>
<span id="cb12-47"><a href="#cb12-47"></a>                    <span class="cf">continue</span></span>
<span id="cb12-48"><a href="#cb12-48"></a></span>
<span id="cb12-49"><a href="#cb12-49"></a>                <span class="va">self</span>.pts_linears.append(nn.Linear(<span class="va">self</span>.Ws[<span class="dv">0</span>], <span class="va">self</span>.Ws[<span class="dv">0</span>]))</span>
<span id="cb12-50"><a href="#cb12-50"></a>            <span class="cf">else</span>:</span>
<span id="cb12-51"><a href="#cb12-51"></a>                <span class="va">self</span>.pts_linears.append(nn.Linear(<span class="va">self</span>.Ws[<span class="dv">2</span>], <span class="va">self</span>.Ws[<span class="dv">2</span>]))</span>
<span id="cb12-52"><a href="#cb12-52"></a></span>
<span id="cb12-53"><a href="#cb12-53"></a>        <span class="co">#         self.pts_linears = nn.ModuleList(</span></span>
<span id="cb12-54"><a href="#cb12-54"></a>        <span class="co">#             [nn.Linear(input_ch, self.W)] + [nn.Linear(self.W, self.W)</span></span>
<span id="cb12-55"><a href="#cb12-55"></a>        <span class="co">#               if i not in self.skips else nn.Linear(self.W + input_ch, self.W)</span></span>
<span id="cb12-56"><a href="#cb12-56"></a>        <span class="co">#                   for i in range(self.D-1)])</span></span>
<span id="cb12-57"><a href="#cb12-57"></a></span>
<span id="cb12-58"><a href="#cb12-58"></a>        <span class="co">### Implementation according to the official code release (https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py#L104-L105)</span></span>
<span id="cb12-59"><a href="#cb12-59"></a>        <span class="va">self</span>.views_linears <span class="op">=</span> nn.ModuleList(</span>
<span id="cb12-60"><a href="#cb12-60"></a>            [nn.Linear(input_ch_views <span class="op">+</span> <span class="va">self</span>.W, <span class="va">self</span>.W <span class="op">//</span> <span class="dv">2</span>)]</span>
<span id="cb12-61"><a href="#cb12-61"></a>        )</span>
<span id="cb12-62"><a href="#cb12-62"></a></span>
<span id="cb12-63"><a href="#cb12-63"></a>        <span class="co">### Implementation according to the paper</span></span>
<span id="cb12-64"><a href="#cb12-64"></a>        <span class="co"># self.views_linears = nn.ModuleList(</span></span>
<span id="cb12-65"><a href="#cb12-65"></a>        <span class="co">#     [nn.Linear(input_ch_views + W, W//2)] + [nn.Linear(W//2, W//2) for i in range(D//2)])</span></span>
<span id="cb12-66"><a href="#cb12-66"></a></span>
<span id="cb12-67"><a href="#cb12-67"></a>        <span class="cf">if</span> use_viewdirs:</span>
<span id="cb12-68"><a href="#cb12-68"></a>            <span class="va">self</span>.feature_linear <span class="op">=</span> nn.Linear(<span class="va">self</span>.W, <span class="va">self</span>.W)</span>
<span id="cb12-69"><a href="#cb12-69"></a>            <span class="va">self</span>.alpha_linear <span class="op">=</span> nn.Linear(<span class="va">self</span>.W, <span class="dv">1</span>)</span>
<span id="cb12-70"><a href="#cb12-70"></a>            <span class="va">self</span>.rgb_linear <span class="op">=</span> nn.Linear(<span class="va">self</span>.W <span class="op">//</span> <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb12-71"><a href="#cb12-71"></a>        <span class="cf">else</span>:</span>
<span id="cb12-72"><a href="#cb12-72"></a>            <span class="va">self</span>.output_linear <span class="op">=</span> nn.Linear(<span class="va">self</span>.W, output_ch)</span>
<span id="cb12-73"><a href="#cb12-73"></a></span>
<span id="cb12-74"><a href="#cb12-74"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-75"><a href="#cb12-75"></a>        input_pts, input_views <span class="op">=</span> torch.split(</span>
<span id="cb12-76"><a href="#cb12-76"></a>            x, [<span class="va">self</span>.input_ch, <span class="va">self</span>.input_ch_views], dim<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb12-77"><a href="#cb12-77"></a>        )</span>
<span id="cb12-78"><a href="#cb12-78"></a>        h <span class="op">=</span> input_pts</span>
<span id="cb12-79"><a href="#cb12-79"></a>        <span class="cf">for</span> i, l <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.pts_linears):</span>
<span id="cb12-80"><a href="#cb12-80"></a>            h <span class="op">=</span> <span class="va">self</span>.pts_linears[i](h)</span>
<span id="cb12-81"><a href="#cb12-81"></a>            h <span class="op">=</span> F.relu(h)</span>
<span id="cb12-82"><a href="#cb12-82"></a>            <span class="cf">if</span> i <span class="kw">in</span> <span class="va">self</span>.skips:</span>
<span id="cb12-83"><a href="#cb12-83"></a>                h <span class="op">=</span> torch.cat([input_pts, h], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-84"><a href="#cb12-84"></a>        <span class="cf">if</span> <span class="va">self</span>.use_viewdirs:</span>
<span id="cb12-85"><a href="#cb12-85"></a>            alpha <span class="op">=</span> <span class="va">self</span>.alpha_linear(h)</span>
<span id="cb12-86"><a href="#cb12-86"></a>            feature <span class="op">=</span> <span class="va">self</span>.feature_linear(h)</span>
<span id="cb12-87"><a href="#cb12-87"></a>            h <span class="op">=</span> torch.cat([feature, input_views], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-88"><a href="#cb12-88"></a></span>
<span id="cb12-89"><a href="#cb12-89"></a>            <span class="cf">for</span> i, l <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.views_linears):</span>
<span id="cb12-90"><a href="#cb12-90"></a>                h <span class="op">=</span> <span class="va">self</span>.views_linears[i](h)</span>
<span id="cb12-91"><a href="#cb12-91"></a>                h <span class="op">=</span> F.relu(h)</span>
<span id="cb12-92"><a href="#cb12-92"></a></span>
<span id="cb12-93"><a href="#cb12-93"></a>            rgb <span class="op">=</span> <span class="va">self</span>.rgb_linear(h)</span>
<span id="cb12-94"><a href="#cb12-94"></a>            outputs <span class="op">=</span> torch.cat([rgb, alpha], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-95"><a href="#cb12-95"></a>        <span class="cf">else</span>:</span>
<span id="cb12-96"><a href="#cb12-96"></a>            outputs <span class="op">=</span> <span class="va">self</span>.output_linear(h)</span>
<span id="cb12-97"><a href="#cb12-97"></a></span>
<span id="cb12-98"><a href="#cb12-98"></a>        <span class="cf">return</span> outputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="creating-a-w23nerf-model" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="creating-a-w23nerf-model"><span class="header-section-number">4.1</span> Creating a <code>W23NeRF</code> model</h2>
<p>We define a <code>make_model_w23()</code> function which creates a <code>W23NeRF</code> model in a manner that satisfies the <code>darwinai.torch.builder.build_model()</code> API and its <code>model_fn</code> argument. It takes in <code>blockspecs</code> which should map to the desired architecture of the original vanilla NeRF Fieldâ€™s base MLP.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="kw">def</span> make_model_w23(blockspecs: BlockSpecs, return_metadata<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb13-2"><a href="#cb13-2"></a>    INPUT_CH <span class="op">=</span> <span class="dv">63</span></span>
<span id="cb13-3"><a href="#cb13-3"></a>    OUTPUT_CH <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb13-4"><a href="#cb13-4"></a>    SKIPS <span class="op">=</span> [<span class="dv">4</span>]</span>
<span id="cb13-5"><a href="#cb13-5"></a>    INPUT_CH_VIEWS <span class="op">=</span> <span class="dv">27</span></span>
<span id="cb13-6"><a href="#cb13-6"></a>    USE_VIEWDIRS <span class="op">=</span> <span class="va">True</span></span>
<span id="cb13-7"><a href="#cb13-7"></a></span>
<span id="cb13-8"><a href="#cb13-8"></a>    model <span class="op">=</span> W23NeRF(</span>
<span id="cb13-9"><a href="#cb13-9"></a>        blockspecs,</span>
<span id="cb13-10"><a href="#cb13-10"></a>        input_ch<span class="op">=</span>INPUT_CH,</span>
<span id="cb13-11"><a href="#cb13-11"></a>        output_ch<span class="op">=</span>OUTPUT_CH,</span>
<span id="cb13-12"><a href="#cb13-12"></a>        skips<span class="op">=</span>SKIPS,</span>
<span id="cb13-13"><a href="#cb13-13"></a>        input_ch_views<span class="op">=</span>INPUT_CH_VIEWS,</span>
<span id="cb13-14"><a href="#cb13-14"></a>        use_viewdirs<span class="op">=</span>USE_VIEWDIRS,</span>
<span id="cb13-15"><a href="#cb13-15"></a>    )</span>
<span id="cb13-16"><a href="#cb13-16"></a></span>
<span id="cb13-17"><a href="#cb13-17"></a>    <span class="cf">if</span> return_metadata:</span>
<span id="cb13-18"><a href="#cb13-18"></a>        <span class="cf">return</span> model, INPUT_CH, INPUT_CH_VIEWS, OUTPUT_CH, SKIPS</span>
<span id="cb13-19"><a href="#cb13-19"></a>    <span class="cf">else</span>:</span>
<span id="cb13-20"><a href="#cb13-20"></a>        <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="reproducing-vanilla-nerf-architecture" class="level2 page-columns page-full" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="reproducing-vanilla-nerf-architecture"><span class="header-section-number">4.2</span> Reproducing Vanilla NeRF architecture</h2>
<section id="expected-initial-blockspecs" class="level3 page-columns page-full" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="expected-initial-blockspecs"><span class="header-section-number">4.2.1</span> Expected initial blockspecs</h3>
<p>The following <code>BlockSpecs</code> should theoretically result in the same model as the vanilla NeRF model.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>initial_blockspecs_to_reproduce_vanilla <span class="op">=</span> BlockSpecs(</span>
<span id="cb14-2"><a href="#cb14-2"></a>    [</span>
<span id="cb14-3"><a href="#cb14-3"></a>        BlockSpec(<span class="dv">256</span>, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb14-4"><a href="#cb14-4"></a>        BlockSpec(<span class="dv">256</span>, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb14-5"><a href="#cb14-5"></a>        BlockSpec(<span class="dv">256</span>, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb14-6"><a href="#cb14-6"></a>    ]</span>
<span id="cb14-7"><a href="#cb14-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The resulting architecture with the above <code>BlockSpecs</code> is shown below:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><code>ModuleList (pts_linear)</code> is the module corresponding to the <code>MLP Base</code> which weâ€™re optimizing. Also while the class is named <code>W23NeRF</code>, itâ€™s more apt to think of it as the <code>Field</code> component of the Vanilla NeRF model, and not the complete NeRF model itself.</p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <code>W23NeRF</code> model is identical to the <code>nerf-pytorch</code> implementation of the vanilla NeRF model. However, this means it suffers from the same <em>potential</em> bug as the <code>nerf-pytorch</code> implementation, where the skip connection is applied at the 6th layer, instead of the 5th layer.</p>
</div>
</div>
<div class="cell page-columns page-full" data-execution_count="15">
<details>
<summary>Detailed architecture summary of <code>W23NeRF</code> model.</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>model_w23, input_ch, input_ch_views, <span class="op">*</span>rest <span class="op">=</span> make_model_w23(</span>
<span id="cb15-2"><a href="#cb15-2"></a>    initial_blockspecs_to_reproduce_vanilla, return_metadata<span class="op">=</span><span class="va">True</span></span>
<span id="cb15-3"><a href="#cb15-3"></a>)</span>
<span id="cb15-4"><a href="#cb15-4"></a>summary(model_w23, input_size<span class="op">=</span>(input_ch <span class="op">+</span> input_ch_views,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display column-page-right" data-execution_count="15">
<pre><code>========================================================================================================================
Layer (type (var_name):depth-idx)        Input Shape      Output Shape     Param #          Param %          Mult-Adds
========================================================================================================================
W23NeRF (W23NeRF)                        [90]             [4]              --                    --          --
â”œâ”€ModuleList (pts_linears): 1-1          --               --               --                    --          --
â”‚    â””â”€Linear (0): 2-1                   [63]             [256]            16,384             2.75%          4,194,304
â”‚    â””â”€Linear (1): 2-2                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (2): 2-3                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (3): 2-4                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (4): 2-5                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (5): 2-6                   [319]            [256]            81,920            13.75%          20,971,520
â”‚    â””â”€Linear (6): 2-7                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (7): 2-8                   [256]            [256]            65,792            11.04%          16,842,752
â”œâ”€Linear (alpha_linear): 1-2             [256]            [1]              257                0.04%          257
â”œâ”€Linear (feature_linear): 1-3           [256]            [256]            65,792            11.04%          16,842,752
â”œâ”€ModuleList (views_linears): 1-4        --               --               --                    --          --
â”‚    â””â”€Linear (0): 2-9                   [283]            [128]            36,352             6.10%          4,653,056
â”œâ”€Linear (rgb_linear): 1-5               [128]            [3]              387                0.06%          1,161
========================================================================================================================
Total params: 595,844
Trainable params: 595,844
Non-trainable params: 0
Total mult-adds (M): 147.72
========================================================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.02
Params size (MB): 2.38
Estimated Total Size (MB): 2.40
========================================================================================================================</code></pre>
</div>
</div>
</section>
<section id="initial-blockspecs-with-large-initial-depths" class="level3 page-columns page-full" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="initial-blockspecs-with-large-initial-depths"><span class="header-section-number">4.2.2</span> Initial BlockSpecs with large initial depths</h3>
<p>Now as an exercise, letâ€™s try modifying the <code>BlockSpecs</code> input so that the initial depths for the first two blocks are arbitrarily large numbers. However, note that</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Potential Bug?
</div>
</div>
<div class="callout-body-container callout-body">
<p>The resulting architecture is identical to the previous one, despite the new large depths specified for block 2 and 3. This indicates that the <code>W23NeRF</code> model does not actually take into account the depths for the first two blocks (this can be verified by looking at the implementation of <code>W23NeRF</code> in <a href="#sec-w23-nerf">Section&nbsp;4</a>).</p>
</div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>initial_blockspecs_test_large <span class="op">=</span> BlockSpecs(</span>
<span id="cb17-2"><a href="#cb17-2"></a>    [</span>
<span id="cb17-3"><a href="#cb17-3"></a>        BlockSpec(<span class="dv">256</span>, <span class="dv">100000</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb17-4"><a href="#cb17-4"></a>        BlockSpec(<span class="dv">256</span>, <span class="dv">23904890238094283</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb17-5"><a href="#cb17-5"></a>        BlockSpec(<span class="dv">256</span>, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb17-6"><a href="#cb17-6"></a>    ]</span>
<span id="cb17-7"><a href="#cb17-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell page-columns page-full" data-execution_count="17">
<details>
<summary>Detailed architecture summary of <code>W23NeRF</code> model.</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>model_w23, input_ch, input_ch_views, <span class="op">*</span>rest <span class="op">=</span> make_model_w23(</span>
<span id="cb18-2"><a href="#cb18-2"></a>    initial_blockspecs_test_large, return_metadata<span class="op">=</span><span class="va">True</span></span>
<span id="cb18-3"><a href="#cb18-3"></a>)</span>
<span id="cb18-4"><a href="#cb18-4"></a>summary(model_w23, input_size<span class="op">=</span>(input_ch <span class="op">+</span> input_ch_views,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display column-page-right" data-execution_count="17">
<pre><code>========================================================================================================================
Layer (type (var_name):depth-idx)        Input Shape      Output Shape     Param #          Param %          Mult-Adds
========================================================================================================================
W23NeRF (W23NeRF)                        [90]             [4]              --                    --          --
â”œâ”€ModuleList (pts_linears): 1-1          --               --               --                    --          --
â”‚    â””â”€Linear (0): 2-1                   [63]             [256]            16,384             2.75%          4,194,304
â”‚    â””â”€Linear (1): 2-2                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (2): 2-3                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (3): 2-4                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (4): 2-5                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (5): 2-6                   [319]            [256]            81,920            13.75%          20,971,520
â”‚    â””â”€Linear (6): 2-7                   [256]            [256]            65,792            11.04%          16,842,752
â”‚    â””â”€Linear (7): 2-8                   [256]            [256]            65,792            11.04%          16,842,752
â”œâ”€Linear (alpha_linear): 1-2             [256]            [1]              257                0.04%          257
â”œâ”€Linear (feature_linear): 1-3           [256]            [256]            65,792            11.04%          16,842,752
â”œâ”€ModuleList (views_linears): 1-4        --               --               --                    --          --
â”‚    â””â”€Linear (0): 2-9                   [283]            [128]            36,352             6.10%          4,653,056
â”œâ”€Linear (rgb_linear): 1-5               [128]            [3]              387                0.06%          1,161
========================================================================================================================
Total params: 595,844
Trainable params: 595,844
Non-trainable params: 0
Total mult-adds (M): 147.72
========================================================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.02
Params size (MB): 2.38
Estimated Total Size (MB): 2.40
========================================================================================================================</code></pre>
</div>
</div>
</section>
<section id="initial-blockspecs-with-small-initial-depth-for-block-3" class="level3 page-columns page-full" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="initial-blockspecs-with-small-initial-depth-for-block-3"><span class="header-section-number">4.2.3</span> Initial blockspecs with small initial depth for block 3</h3>
<p>However, if we modify the initial depth for block 3, we see that the resulting architecture is different. This indicates that the <code>W23NeRF</code> model does take into account the depth for the third block.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>initial_blockspecs_test_small_block3_depth <span class="op">=</span> BlockSpecs(</span>
<span id="cb20-2"><a href="#cb20-2"></a>    [</span>
<span id="cb20-3"><a href="#cb20-3"></a>        BlockSpec(<span class="dv">256</span>, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb20-4"><a href="#cb20-4"></a>        BlockSpec(<span class="dv">256</span>, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb20-5"><a href="#cb20-5"></a>        BlockSpec(<span class="dv">256</span>, <span class="dv">4</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb20-6"><a href="#cb20-6"></a>    ]</span>
<span id="cb20-7"><a href="#cb20-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell page-columns page-full" data-execution_count="19">
<details>
<summary>Detailed architecture summary of <code>W23NeRF</code> model.</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>model_w23, input_ch, input_ch_views, <span class="op">*</span>rest <span class="op">=</span> make_model_w23(</span>
<span id="cb21-2"><a href="#cb21-2"></a>    initial_blockspecs_test_small_block3_depth, return_metadata<span class="op">=</span><span class="va">True</span></span>
<span id="cb21-3"><a href="#cb21-3"></a>)</span>
<span id="cb21-4"><a href="#cb21-4"></a>summary(model_w23, input_size<span class="op">=</span>(input_ch <span class="op">+</span> input_ch_views,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display column-page-right" data-execution_count="19">
<pre><code>========================================================================================================================
Layer (type (var_name):depth-idx)        Input Shape      Output Shape     Param #          Param %          Mult-Adds
========================================================================================================================
W23NeRF (W23NeRF)                        [90]             [4]              --                    --          --
â”œâ”€ModuleList (pts_linears): 1-1          --               --               --                    --          --
â”‚    â””â”€Linear (0): 2-1                   [63]             [256]            16,384             4.92%          4,194,304
â”‚    â””â”€Linear (1): 2-2                   [256]            [256]            65,792            19.78%          16,842,752
â”‚    â””â”€Linear (2): 2-3                   [256]            [256]            65,792            19.78%          16,842,752
â”‚    â””â”€Linear (3): 2-4                   [319]            [256]            81,920            24.62%          20,971,520
â”œâ”€Linear (alpha_linear): 1-2             [256]            [1]              257                0.08%          257
â”œâ”€Linear (feature_linear): 1-3           [256]            [256]            65,792            19.78%          16,842,752
â”œâ”€ModuleList (views_linears): 1-4        --               --               --                    --          --
â”‚    â””â”€Linear (0): 2-5                   [283]            [128]            36,352            10.93%          4,653,056
â”œâ”€Linear (rgb_linear): 1-5               [128]            [3]              387                0.12%          1,161
========================================================================================================================
Total params: 332,676
Trainable params: 332,676
Non-trainable params: 0
Total mult-adds (M): 80.35
========================================================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 1.33
Estimated Total Size (MB): 1.34
========================================================================================================================</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="sec-gen-nerf" class="level1 page-columns page-full" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Gen-NeRF Vanilla Model (<code>GenNerf</code>)</h1>
<div class="cell" data-execution_count="20">
<details>
<summary>Architecture summary of <code>GenNerf</code> model.</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>gen_nerf_pipeline_cfg <span class="op">=</span> GenNerfPipelineConfig()</span>
<span id="cb23-2"><a href="#cb23-2"></a>gen_nerf_cfg <span class="op">=</span> GenNerfModelConfig()</span>
<span id="cb23-3"><a href="#cb23-3"></a>scene_box <span class="op">=</span> SceneBox(</span>
<span id="cb23-4"><a href="#cb23-4"></a>    aabb<span class="op">=</span>torch.tensor([[<span class="op">-</span><span class="fl">1.5</span>, <span class="op">-</span><span class="fl">1.5</span>, <span class="op">-</span><span class="fl">1.5</span>], [<span class="fl">1.5</span>, <span class="fl">1.5</span>, <span class="fl">1.5</span>]], dtype<span class="op">=</span>torch.float32)</span>
<span id="cb23-5"><a href="#cb23-5"></a>)</span>
<span id="cb23-6"><a href="#cb23-6"></a>gen_nerf <span class="op">=</span> gen_nerf_cfg.setup(</span>
<span id="cb23-7"><a href="#cb23-7"></a>    scene_box<span class="op">=</span>scene_box,</span>
<span id="cb23-8"><a href="#cb23-8"></a>    num_train_data<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb23-9"><a href="#cb23-9"></a>)</span>
<span id="cb23-10"><a href="#cb23-10"></a>summary(gen_nerf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>============================================================================
Layer (type (var_name):depth-idx)                            Param #
============================================================================
GenNerfModel (GenNerfModel)                                  --
â”œâ”€NearFarCollider (collider): 1-1                            --
â”œâ”€GenNerfField (field_coarse): 1-2                           --
â”‚    â””â”€NeRFEncoding (position_encoding): 2-1                 --
â”‚    â””â”€NeRFEncoding (direction_encoding): 2-2                --
â”‚    â””â”€CompressibleMLP (mlp_base): 2-3                       --
â”‚    â”‚    â””â”€ReLU (activation): 3-1                           --
â”‚    â”‚    â””â”€ReLU (out_activation): 3-2                       --
â”‚    â”‚    â””â”€ModuleList (layers): 3-3                         --
â”‚    â”‚    â”‚    â””â”€Linear (0): 4-1                             16,384
â”‚    â”‚    â”‚    â””â”€Linear (1): 4-2                             65,792
â”‚    â”‚    â”‚    â””â”€Linear (2): 4-3                             65,792
â”‚    â”‚    â”‚    â””â”€Linear (3): 4-4                             65,792
â”‚    â”‚    â”‚    â””â”€Linear (4): 4-5                             81,920
â”‚    â”‚    â”‚    â””â”€Linear (5): 4-6                             65,792
â”‚    â”‚    â”‚    â””â”€Linear (6): 4-7                             65,792
â”‚    â”‚    â”‚    â””â”€Linear (7): 4-8                             65,792
â”‚    â””â”€MLP (mlp_head): 2-4                                   --
â”‚    â”‚    â””â”€ReLU (activation): 3-4                           --
â”‚    â”‚    â””â”€ReLU (out_activation): 3-5                       --
â”‚    â”‚    â””â”€ModuleList (layers): 3-6                         --
â”‚    â”‚    â”‚    â””â”€Linear (0): 4-9                             36,352
â”‚    â”‚    â”‚    â””â”€Linear (1): 4-10                            16,512
â”‚    â””â”€DensityFieldHead (field_output_density): 2-5          --
â”‚    â”‚    â””â”€Softplus (activation): 3-7                       --
â”‚    â”‚    â””â”€Linear (net): 3-8                                257
â”‚    â””â”€ModuleList (field_heads): 2-6                         --
â”‚    â”‚    â””â”€RGBFieldHead (0): 3-9                            --
â”‚    â”‚    â”‚    â””â”€Sigmoid (activation): 4-11                  --
â”‚    â”‚    â”‚    â””â”€Linear (net): 4-12                          387
â”œâ”€GenNerfField (field_fine): 1-3                             --
â”‚    â””â”€NeRFEncoding (position_encoding): 2-7                 --
â”‚    â””â”€NeRFEncoding (direction_encoding): 2-8                --
â”‚    â””â”€CompressibleMLP (mlp_base): 2-9                       --
â”‚    â”‚    â””â”€ReLU (activation): 3-10                          --
â”‚    â”‚    â””â”€ReLU (out_activation): 3-11                      --
â”‚    â”‚    â””â”€ModuleList (layers): 3-12                        --
â”‚    â”‚    â”‚    â””â”€Linear (0): 4-13                            16,384
â”‚    â”‚    â”‚    â””â”€Linear (1): 4-14                            65,792
â”‚    â”‚    â”‚    â””â”€Linear (2): 4-15                            65,792
â”‚    â”‚    â”‚    â””â”€Linear (3): 4-16                            65,792
â”‚    â”‚    â”‚    â””â”€Linear (4): 4-17                            81,920
â”‚    â”‚    â”‚    â””â”€Linear (5): 4-18                            65,792
â”‚    â”‚    â”‚    â””â”€Linear (6): 4-19                            65,792
â”‚    â”‚    â”‚    â””â”€Linear (7): 4-20                            65,792
â”‚    â””â”€MLP (mlp_head): 2-10                                  --
â”‚    â”‚    â””â”€ReLU (activation): 3-13                          --
â”‚    â”‚    â””â”€ReLU (out_activation): 3-14                      --
â”‚    â”‚    â””â”€ModuleList (layers): 3-15                        --
â”‚    â”‚    â”‚    â””â”€Linear (0): 4-21                            36,352
â”‚    â”‚    â”‚    â””â”€Linear (1): 4-22                            16,512
â”‚    â””â”€DensityFieldHead (field_output_density): 2-11         --
â”‚    â”‚    â””â”€Softplus (activation): 3-16                      --
â”‚    â”‚    â””â”€Linear (net): 3-17                               257
â”‚    â””â”€ModuleList (field_heads): 2-12                        387
â”‚    â”‚    â””â”€RGBFieldHead (0): 3-18                           (recursive)
â”‚    â”‚    â”‚    â””â”€Sigmoid (activation): 4-23                  --
â”‚    â”‚    â”‚    â””â”€Linear (net): 4-24                          (recursive)
â”œâ”€UniformSampler (sampler_uniform): 1-4                      --
â”œâ”€PDFSampler (sampler_pdf): 1-5                              --
â”œâ”€RGBRenderer (renderer_rgb): 1-6                            --
â”œâ”€AccumulationRenderer (renderer_accumulation): 1-7          --
â”œâ”€DepthRenderer (renderer_depth): 1-8                        --
â”œâ”€MSELoss (rgb_loss): 1-9                                    --
â”œâ”€PeakSignalNoiseRatio (psnr): 1-10                          --
â”œâ”€LearnedPerceptualImagePatchSimilarity (lpips): 1-11        --
â”‚    â””â”€NoTrainLpips (net): 2-13                              --
â”‚    â”‚    â””â”€ScalingLayer (scaling_layer): 3-19               --
â”‚    â”‚    â””â”€alexnet (net): 3-20                              --
â”‚    â”‚    â”‚    â””â”€Sequential (slice1): 4-25                   (23,296)
â”‚    â”‚    â”‚    â””â”€Sequential (slice2): 4-26                   (307,392)
â”‚    â”‚    â”‚    â””â”€Sequential (slice3): 4-27                   (663,936)
â”‚    â”‚    â”‚    â””â”€Sequential (slice4): 4-28                   (884,992)
â”‚    â”‚    â”‚    â””â”€Sequential (slice5): 4-29                   (590,080)
â”‚    â”‚    â””â”€NetLinLayer (lin0): 3-21                         --
â”‚    â”‚    â”‚    â””â”€Sequential (model): 4-30                    64
â”‚    â”‚    â””â”€NetLinLayer (lin1): 3-22                         --
â”‚    â”‚    â”‚    â””â”€Sequential (model): 4-31                    192
â”‚    â”‚    â””â”€NetLinLayer (lin2): 3-23                         --
â”‚    â”‚    â”‚    â””â”€Sequential (model): 4-32                    384
â”‚    â”‚    â””â”€NetLinLayer (lin3): 3-24                         --
â”‚    â”‚    â”‚    â””â”€Sequential (model): 4-33                    256
â”‚    â”‚    â””â”€NetLinLayer (lin4): 3-25                         --
â”‚    â”‚    â”‚    â””â”€Sequential (model): 4-34                    256
â”‚    â”‚    â””â”€ModuleList (lins): 3-26                          1,152
â”‚    â”‚    â”‚    â””â”€NetLinLayer (0): 4-35                       (recursive)
â”‚    â”‚    â”‚    â””â”€NetLinLayer (1): 4-36                       (recursive)
â”‚    â”‚    â”‚    â””â”€NetLinLayer (2): 4-37                       (recursive)
â”‚    â”‚    â”‚    â””â”€NetLinLayer (3): 4-38                       (recursive)
â”‚    â”‚    â”‚    â””â”€NetLinLayer (4): 4-39                       (recursive)
============================================================================
Total params: 3,565,128
Trainable params: 1,095,432
Non-trainable params: 2,469,696
============================================================================</code></pre>
</div>
</div>
<section id="coarse-field-base-mlp-architecture-2" class="level2 page-columns page-full" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="coarse-field-base-mlp-architecture-2"><span class="header-section-number">5.1</span> Coarse Field Base MLP Architecture</h2>
<div class="cell page-columns page-full" data-execution_count="21">
<details>
<summary>Detailed architecture summary for <code>GenNerf</code> coarse field base MLP</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>summary(gen_nerf.field_coarse.mlp_base, input_size<span class="op">=</span>(<span class="dv">63</span>,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="gen_nerf-coarse-base-mlp-summary" class="cell-output cell-output-display column-page-right" data-execution_count="21">
<pre><code>========================================================================================================================
Layer (type (var_name):depth-idx)        Input Shape      Output Shape     Param #          Param %          Mult-Adds
========================================================================================================================
CompressibleMLP (CompressibleMLP)        [63]             [256]            --                    --          --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (0): 2-1                   [63]             [256]            16,384             3.32%          4,194,304
â”œâ”€ReLU (activation): 1-2                 [256]            [256]            --                    --          --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (1): 2-2                   [256]            [256]            65,792            13.34%          16,842,752
â”œâ”€ReLU (activation): 1-4                 [256]            [256]            --               (recursive)      --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (2): 2-3                   [256]            [256]            65,792            13.34%          16,842,752
â”œâ”€ReLU (activation): 1-6                 [256]            [256]            --               (recursive)      --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (3): 2-4                   [256]            [256]            65,792            13.34%          16,842,752
â”œâ”€ReLU (activation): 1-8                 [256]            [256]            --               (recursive)      --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (4): 2-5                   [319]            [256]            81,920            16.61%          20,971,520
â”œâ”€ReLU (activation): 1-10                [256]            [256]            --               (recursive)      --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (5): 2-6                   [256]            [256]            65,792            13.34%          16,842,752
â”œâ”€ReLU (activation): 1-12                [256]            [256]            --               (recursive)      --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (6): 2-7                   [256]            [256]            65,792            13.34%          16,842,752
â”œâ”€ReLU (activation): 1-14                [256]            [256]            --               (recursive)      --
â”œâ”€ModuleList (layers): 1-15              --               --               (recursive)      (recursive)      --
â”‚    â””â”€Linear (7): 2-8                   [256]            [256]            65,792            13.34%          16,842,752
â”œâ”€ReLU (out_activation): 1-16            [256]            [256]            --                    --          --
========================================================================================================================
Total params: 493,056
Trainable params: 493,056
Non-trainable params: 0
Total mult-adds (M): 126.22
========================================================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.02
Params size (MB): 1.97
Estimated Total Size (MB): 1.99
========================================================================================================================</code></pre>
</div>
</div>
</section>
<section id="reproducing-vanilla-nerf-architecture-1" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="reproducing-vanilla-nerf-architecture-1"><span class="header-section-number">5.2</span> Reproducing Vanilla NeRF architecture</h2>
<p>Letâ€™s validate that the default <code>GenNerf</code> model base MLP is identical to the <code>VanillaNeRF</code> model.</p>
<div class="cell" data-execution_count="22">
<details>
<summary>Test case demonstrating that default <code>GenNerf</code> matches <code>VanillaNerf</code> implementation for the base MLP</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>vanilla_pipeline_cfg <span class="op">=</span> VanillaPipelineConfig()</span>
<span id="cb27-2"><a href="#cb27-2"></a>vanilla_nerf_cfg <span class="op">=</span> VanillaModelConfig()</span>
<span id="cb27-3"><a href="#cb27-3"></a>dummy_datamanager <span class="op">=</span> vanilla_pipeline_cfg.datamanager</span>
<span id="cb27-4"><a href="#cb27-4"></a>scene_box <span class="op">=</span> SceneBox(</span>
<span id="cb27-5"><a href="#cb27-5"></a>    aabb<span class="op">=</span>torch.tensor([[<span class="op">-</span><span class="fl">1.5</span>, <span class="op">-</span><span class="fl">1.5</span>, <span class="op">-</span><span class="fl">1.5</span>], [<span class="fl">1.5</span>, <span class="fl">1.5</span>, <span class="fl">1.5</span>]], dtype<span class="op">=</span>torch.float32)</span>
<span id="cb27-6"><a href="#cb27-6"></a>)</span>
<span id="cb27-7"><a href="#cb27-7"></a>vanilla_nerf <span class="op">=</span> vanilla_nerf_cfg.setup(</span>
<span id="cb27-8"><a href="#cb27-8"></a>    scene_box<span class="op">=</span>scene_box,</span>
<span id="cb27-9"><a href="#cb27-9"></a>    num_train_data<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb27-10"><a href="#cb27-10"></a>).to(device<span class="op">=</span><span class="st">"cpu"</span>)</span>
<span id="cb27-11"><a href="#cb27-11"></a></span>
<span id="cb27-12"><a href="#cb27-12"></a>gen_nerf.to(device<span class="op">=</span><span class="st">"cpu"</span>)</span>
<span id="cb27-13"><a href="#cb27-13"></a>test_module_lists_equal(</span>
<span id="cb27-14"><a href="#cb27-14"></a>    gen_nerf.field_coarse.mlp_base.layers, vanilla_nerf.field_coarse.mlp_base.layers</span>
<span id="cb27-15"><a href="#cb27-15"></a>)</span>
<span id="cb27-16"><a href="#cb27-16"></a></span>
<span id="cb27-17"><a href="#cb27-17"></a>input_shape <span class="op">=</span> [<span class="dv">63</span>]</span>
<span id="cb27-18"><a href="#cb27-18"></a>baseline_flops <span class="op">=</span> get_flops(vanilla_nerf.field_coarse.mlp_base, input_shape)</span>
<span id="cb27-19"><a href="#cb27-19"></a>baseline_params <span class="op">=</span> get_params(vanilla_nerf.field_coarse.mlp_base)</span>
<span id="cb27-20"><a href="#cb27-20"></a></span>
<span id="cb27-21"><a href="#cb27-21"></a>gen_nerf_base_mlp_flops <span class="op">=</span> get_flops(gen_nerf.field_coarse.mlp_base, input_shape)</span>
<span id="cb27-22"><a href="#cb27-22"></a>gen_nerf_base_mlp_params <span class="op">=</span> get_params(gen_nerf.field_coarse.mlp_base)</span>
<span id="cb27-23"><a href="#cb27-23"></a></span>
<span id="cb27-24"><a href="#cb27-24"></a><span class="co"># Validate that GenNerf MLP Base matches VanillaNerf MLP Base flops and params</span></span>
<span id="cb27-25"><a href="#cb27-25"></a><span class="cf">assert</span> (</span>
<span id="cb27-26"><a href="#cb27-26"></a>    baseline_flops <span class="op">==</span> gen_nerf_base_mlp_flops</span>
<span id="cb27-27"><a href="#cb27-27"></a>), <span class="ss">f"GenNerf MLP Base flops </span><span class="sc">{</span>gen_nerf_base_mlp_flops<span class="sc">}</span><span class="ss"> != VanillaNerf MLP Base flops </span><span class="sc">{</span>baseline_flops<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb27-28"><a href="#cb27-28"></a></span>
<span id="cb27-29"><a href="#cb27-29"></a><span class="cf">assert</span> (</span>
<span id="cb27-30"><a href="#cb27-30"></a>    baseline_params <span class="op">==</span> gen_nerf_base_mlp_params</span>
<span id="cb27-31"><a href="#cb27-31"></a>), <span class="ss">f"GenNerf MLP Base params </span><span class="sc">{</span>gen_nerf_base_mlp_params<span class="sc">}</span><span class="ss"> != VanillaNerf MLP Base params </span><span class="sc">{</span>baseline_params<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb27-32"><a href="#cb27-32"></a></span>
<span id="cb27-33"><a href="#cb27-33"></a><span class="bu">print</span>(<span class="ss">f"GenNerf MLP Base matches VanillaNerf MLP Base!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>GenNerf MLP Base matches VanillaNerf MLP Base!</code></pre>
</div>
</div>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a>get_flops(</span>
<span id="cb29-2"><a href="#cb29-2"></a>    vanilla_nerf.field_coarse.mlp_base,</span>
<span id="cb29-3"><a href="#cb29-3"></a>    input_shape,</span>
<span id="cb29-4"><a href="#cb29-4"></a>    verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb29-5"><a href="#cb29-5"></a>    ret_layer_info<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb29-6"><a href="#cb29-6"></a>    report_missing<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb29-7"><a href="#cb29-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Register zero_ops() for &lt;class 'torch.nn.modules.activation.ReLU'&gt;.
[INFO] Register count_linear() for &lt;class 'torch.nn.modules.linear.Linear'&gt;.
[WARN] Cannot find rule for &lt;class 'torch.nn.modules.container.ModuleList'&gt;. Treat it as zero Macs and zero Params.
[WARN] Cannot find rule for &lt;class 'nerfstudio.field_components.mlp.MLP'&gt;. Treat it as zero Macs and zero Params.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>(491008.0,
 493056.0,
 {'activation': (0.0, 0.0, {}),
  'out_activation': (0.0, 0.0, {}),
  'layers': (491008.0,
   493056.0,
   {'0': (16128.0, 16384.0, {}),
    '1': (65536.0, 65792.0, {}),
    '2': (65536.0, 65792.0, {}),
    '3': (65536.0, 65792.0, {}),
    '4': (81664.0, 81920.0, {}),
    '5': (65536.0, 65792.0, {}),
    '6': (65536.0, 65792.0, {}),
    '7': (65536.0, 65792.0, {})})})</code></pre>
</div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="bu">print</span>(<span class="op">*</span>vanilla_nerf.field_coarse.mlp_base.named_children())</span>
<span id="cb32-2"><a href="#cb32-2"></a></span>
<span id="cb32-3"><a href="#cb32-3"></a><span class="cf">for</span> n, m <span class="kw">in</span> vanilla_nerf.field_coarse.mlp_base.named_children():</span>
<span id="cb32-4"><a href="#cb32-4"></a>    <span class="cf">if</span> n <span class="op">==</span> <span class="st">"layers"</span>:</span>
<span id="cb32-5"><a href="#cb32-5"></a>        <span class="bu">print</span>(<span class="ss">f"n: </span><span class="sc">{</span>m<span class="sc">.</span>total_ops<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>m<span class="sc">.</span>total_params<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>('activation', ReLU()) ('out_activation', ReLU()) ('layers', ModuleList(
  (0): Linear(in_features=63, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=256, bias=True)
  (4): Linear(in_features=319, out_features=256, bias=True)
  (5): Linear(in_features=256, out_features=256, bias=True)
  (6): Linear(in_features=256, out_features=256, bias=True)
  (7): Linear(in_features=256, out_features=256, bias=True)
))
n: 0.0, 0.0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="bu">print</span>(<span class="op">*</span>gen_nerf.field_coarse.mlp_base.named_children())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>('activation', ReLU()) ('out_activation', ReLU()) ('layers', ModuleList(
  (0): Linear(in_features=63, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=256, bias=True)
  (4): Linear(in_features=319, out_features=256, bias=True)
  (5): Linear(in_features=256, out_features=256, bias=True)
  (6): Linear(in_features=256, out_features=256, bias=True)
  (7): Linear(in_features=256, out_features=256, bias=True)
))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>get_flops(</span>
<span id="cb36-2"><a href="#cb36-2"></a>    gen_nerf.field_coarse.mlp_base,</span>
<span id="cb36-3"><a href="#cb36-3"></a>    input_shape,</span>
<span id="cb36-4"><a href="#cb36-4"></a>    verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb36-5"><a href="#cb36-5"></a>    ret_layer_info<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb36-6"><a href="#cb36-6"></a>    report_missing<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb36-7"><a href="#cb36-7"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Register zero_ops() for &lt;class 'torch.nn.modules.activation.ReLU'&gt;.
[INFO] Register count_linear() for &lt;class 'torch.nn.modules.linear.Linear'&gt;.
[WARN] Cannot find rule for &lt;class 'torch.nn.modules.container.ModuleList'&gt;. Treat it as zero Macs and zero Params.
[WARN] Cannot find rule for &lt;class 'gen_nerf.modules.mlp.CompressibleMLP'&gt;. Treat it as zero Macs and zero Params.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>(491008.0,
 493056.0,
 {'activation': (0.0, 0.0, {}),
  'out_activation': (0.0, 0.0, {}),
  'layers': (491008.0,
   493056.0,
   {'0': (16128.0, 16384.0, {}),
    '1': (65536.0, 65792.0, {}),
    '2': (65536.0, 65792.0, {}),
    '3': (65536.0, 65792.0, {}),
    '4': (81664.0, 81920.0, {}),
    '5': (65536.0, 65792.0, {}),
    '6': (65536.0, 65792.0, {}),
    '7': (65536.0, 65792.0, {})})})</code></pre>
</div>
</div>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a>coarse_model, fine_model, grad_vars, input_ch, input_ch_views <span class="op">=</span> create_nerf()</span>
<span id="cb39-2"><a href="#cb39-2"></a>get_flops(</span>
<span id="cb39-3"><a href="#cb39-3"></a>    coarse_model,</span>
<span id="cb39-4"><a href="#cb39-4"></a>    [<span class="dv">63</span> <span class="op">+</span> <span class="dv">27</span>],</span>
<span id="cb39-5"><a href="#cb39-5"></a>    verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb39-6"><a href="#cb39-6"></a>    ret_layer_info<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb39-7"><a href="#cb39-7"></a>    report_missing<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb39-8"><a href="#cb39-8"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Register count_linear() for &lt;class 'torch.nn.modules.linear.Linear'&gt;.
[WARN] Cannot find rule for &lt;class 'torch.nn.modules.container.ModuleList'&gt;. Treat it as zero Macs and zero Params.
[WARN] Cannot find rule for &lt;class '__main__.PyTorchNeRF'&gt;. Treat it as zero Macs and zero Params.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>(593408.0,
 595844.0,
 {'pts_linears': (491008.0,
   493056.0,
   {'0': (16128.0, 16384.0, {}),
    '1': (65536.0, 65792.0, {}),
    '2': (65536.0, 65792.0, {}),
    '3': (65536.0, 65792.0, {}),
    '4': (65536.0, 65792.0, {}),
    '5': (81664.0, 81920.0, {}),
    '6': (65536.0, 65792.0, {}),
    '7': (65536.0, 65792.0, {})}),
  'views_linears': (36224.0, 36352.0, {'0': (36224.0, 36352.0, {})}),
  'feature_linear': (65536.0, 65792.0, {}),
  'alpha_linear': (256.0, 257.0, {}),
  'rgb_linear': (384.0, 387.0, {})})</code></pre>
</div>
</div>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a>summary(gen_nerf.field_coarse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>==================================================================
Layer (type (var_name):depth-idx)                  Param #
==================================================================
GenNerfField (GenNerfField)                        --
â”œâ”€NeRFEncoding (position_encoding): 1-1            --
â”œâ”€NeRFEncoding (direction_encoding): 1-2           --
â”œâ”€CompressibleMLP (mlp_base): 1-3                  --
â”‚    â””â”€ReLU (activation): 2-1                      --
â”‚    â””â”€ReLU (out_activation): 2-2                  --
â”‚    â””â”€ModuleList (layers): 2-3                    --
â”‚    â”‚    â””â”€Linear (0): 3-1                        16,384
â”‚    â”‚    â””â”€Linear (1): 3-2                        65,792
â”‚    â”‚    â””â”€Linear (2): 3-3                        65,792
â”‚    â”‚    â””â”€Linear (3): 3-4                        65,792
â”‚    â”‚    â””â”€Linear (4): 3-5                        81,920
â”‚    â”‚    â””â”€Linear (5): 3-6                        65,792
â”‚    â”‚    â””â”€Linear (6): 3-7                        65,792
â”‚    â”‚    â””â”€Linear (7): 3-8                        65,792
â”œâ”€MLP (mlp_head): 1-4                              --
â”‚    â””â”€ReLU (activation): 2-4                      --
â”‚    â””â”€ReLU (out_activation): 2-5                  --
â”‚    â””â”€ModuleList (layers): 2-6                    --
â”‚    â”‚    â””â”€Linear (0): 3-9                        36,352
â”‚    â”‚    â””â”€Linear (1): 3-10                       16,512
â”œâ”€DensityFieldHead (field_output_density): 1-5     --
â”‚    â””â”€Softplus (activation): 2-7                  --
â”‚    â””â”€Linear (net): 2-8                           257
â”œâ”€ModuleList (field_heads): 1-6                    --
â”‚    â””â”€RGBFieldHead (0): 2-9                       --
â”‚    â”‚    â””â”€Sigmoid (activation): 3-11             --
â”‚    â”‚    â””â”€Linear (net): 3-12                     387
==================================================================
Total params: 546,564
Trainable params: 546,564
Non-trainable params: 0
==================================================================</code></pre>
</div>
</div>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a>summary(vanilla_nerf.field_coarse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>==================================================================
Layer (type (var_name):depth-idx)                  Param #
==================================================================
NeRFField (NeRFField)                              --
â”œâ”€NeRFEncoding (position_encoding): 1-1            --
â”œâ”€NeRFEncoding (direction_encoding): 1-2           --
â”œâ”€MLP (mlp_base): 1-3                              --
â”‚    â””â”€ReLU (activation): 2-1                      --
â”‚    â””â”€ReLU (out_activation): 2-2                  --
â”‚    â””â”€ModuleList (layers): 2-3                    --
â”‚    â”‚    â””â”€Linear (0): 3-1                        16,384
â”‚    â”‚    â””â”€Linear (1): 3-2                        65,792
â”‚    â”‚    â””â”€Linear (2): 3-3                        65,792
â”‚    â”‚    â””â”€Linear (3): 3-4                        65,792
â”‚    â”‚    â””â”€Linear (4): 3-5                        81,920
â”‚    â”‚    â””â”€Linear (5): 3-6                        65,792
â”‚    â”‚    â””â”€Linear (6): 3-7                        65,792
â”‚    â”‚    â””â”€Linear (7): 3-8                        65,792
â”œâ”€MLP (mlp_head): 1-4                              --
â”‚    â””â”€ReLU (activation): 2-4                      --
â”‚    â””â”€ReLU (out_activation): 2-5                  --
â”‚    â””â”€ModuleList (layers): 2-6                    --
â”‚    â”‚    â””â”€Linear (0): 3-9                        36,352
â”‚    â”‚    â””â”€Linear (1): 3-10                       16,512
â”œâ”€DensityFieldHead (field_output_density): 1-5     --
â”‚    â””â”€Softplus (activation): 2-7                  --
â”‚    â””â”€Linear (net): 2-8                           257
â”œâ”€ModuleList (field_heads): 1-6                    --
â”‚    â””â”€RGBFieldHead (0): 2-9                       --
â”‚    â”‚    â””â”€Sigmoid (activation): 3-11             --
â”‚    â”‚    â””â”€Linear (net): 3-12                     387
==================================================================
Total params: 546,564
Trainable params: 546,564
Non-trainable params: 0
==================================================================</code></pre>
</div>
</div>
</section>
<section id="initial-blockspecs-to-generate-vanilla-nerf-architecture" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="initial-blockspecs-to-generate-vanilla-nerf-architecture"><span class="header-section-number">5.3</span> Initial BlockSpecs to generate Vanilla NeRF architecture</h2>
<div class="cell" data-execution_count="30">
<details>
<summary>BlockSpecs for creating an MLP Base for <code>GenNerf</code> that matches NerfStudioâ€™s <code>VanillaNerf</code> implementation</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a>mlp_blocks_searcher: MLPBlocksSearcher <span class="op">=</span> MLPBlocksSearcherConfig().setup(</span>
<span id="cb46-2"><a href="#cb46-2"></a>    input_shape<span class="op">=</span>[<span class="dv">63</span>],</span>
<span id="cb46-3"><a href="#cb46-3"></a>    in_dim<span class="op">=</span><span class="dv">63</span>,</span>
<span id="cb46-4"><a href="#cb46-4"></a>    out_activation<span class="op">=</span>nn.ReLU(),</span>
<span id="cb46-5"><a href="#cb46-5"></a>)</span>
<span id="cb46-6"><a href="#cb46-6"></a>initial_blockspecs_to_reproduce_vanilla: MLPBlockSpecsConfig <span class="op">=</span> (</span>
<span id="cb46-7"><a href="#cb46-7"></a>    mlp_blocks_searcher.get_blockspecs_config(BaseMLPArchStyle.VANILLA)</span>
<span id="cb46-8"><a href="#cb46-8"></a>)</span>
<span id="cb46-9"><a href="#cb46-9"></a><span class="bu">print</span>(initial_blockspecs_to_reproduce_vanilla)</span>
<span id="cb46-10"><a href="#cb46-10"></a></span>
<span id="cb46-11"><a href="#cb46-11"></a>base_mlp_to_reproduce_vanilla <span class="op">=</span> mlp_blocks_searcher.generate_model(</span>
<span id="cb46-12"><a href="#cb46-12"></a>    initial_blockspecs_to_reproduce_vanilla.setup()</span>
<span id="cb46-13"><a href="#cb46-13"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>MLPBlockSpecsConfig:
    _target: &lt;class 'gen_nerf.modules.mlp.MLPBlockSpecs'&gt;
    stage1: BlockSpecConfig(_target=&lt;class 'darwinai.torch.builder.BlockSpec'&gt;, channels=256, depth=4, freeze_channel=False, freeze_depth=False)
    stage2: BlockSpecConfig(_target=&lt;class 'darwinai.torch.builder.BlockSpec'&gt;, channels=256, depth=1, freeze_channel=False, freeze_depth=True)
    stage3: BlockSpecConfig(_target=&lt;class 'darwinai.torch.builder.BlockSpec'&gt;, channels=256, depth=3, freeze_channel=False, freeze_depth=False)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="31">
<details>
<summary>Test case demonstrating that the blockspecs registered to <code>MLPBlocksSearcher</code> for <code>BaseMLPArchStyle.VANILLA</code> match NerfStudioâ€™s <code>VanillaNerf</code> MLP Base architecture</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1"></a>test_module_lists_equal(</span>
<span id="cb48-2"><a href="#cb48-2"></a>    base_mlp_to_reproduce_vanilla.layers, vanilla_nerf.field_coarse.mlp_base.layers</span>
<span id="cb48-3"><a href="#cb48-3"></a>)</span>
<span id="cb48-4"><a href="#cb48-4"></a><span class="bu">print</span>(<span class="ss">f"GenNerf MLP Base matches VanillaNerf MLP Base!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>GenNerf MLP Base matches VanillaNerf MLP Base!</code></pre>
</div>
</div>
</section>
<section id="modifying-target-ratio-for-initial-blockspecs" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="modifying-target-ratio-for-initial-blockspecs"><span class="header-section-number">5.4</span> Modifying Target Ratio for Initial Blockspecs</h2>
<section id="validating-that-target-ratio-of-1.0-results-in-identical-architecture-to-vanillanerf-base-mlp" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="validating-that-target-ratio-of-1.0-results-in-identical-architecture-to-vanillanerf-base-mlp"><span class="header-section-number">5.4.1</span> Validating that target ratio of 1.0 results in identical architecture to <code>VanillaNeRF</code> Base MLP</h3>
<div class="cell" data-execution_count="44">
<details>
<summary>BlockSpecs for creating an MLP Base with target flops ratio of 1</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1"></a>mlp_blocks_searcher_cfg <span class="op">=</span> MLPBlocksSearcherConfig(</span>
<span id="cb50-2"><a href="#cb50-2"></a>    target_ratio<span class="op">=</span><span class="fl">1.0</span>, arch_style<span class="op">=</span>BaseMLPArchStyle.LARGE_UNIFORM</span>
<span id="cb50-3"><a href="#cb50-3"></a>)</span>
<span id="cb50-4"><a href="#cb50-4"></a>mlp_blocks_searcher: MLPBlocksSearcher <span class="op">=</span> mlp_blocks_searcher_cfg.setup(</span>
<span id="cb50-5"><a href="#cb50-5"></a>    input_shape<span class="op">=</span>[<span class="dv">63</span>],</span>
<span id="cb50-6"><a href="#cb50-6"></a>    in_dim<span class="op">=</span><span class="dv">63</span>,</span>
<span id="cb50-7"><a href="#cb50-7"></a>    out_activation<span class="op">=</span>nn.ReLU(),</span>
<span id="cb50-8"><a href="#cb50-8"></a>)</span>
<span id="cb50-9"><a href="#cb50-9"></a>generated_model <span class="op">=</span> mlp_blocks_searcher.generated_model</span>
<span id="cb50-10"><a href="#cb50-10"></a>test_module_lists_equal(</span>
<span id="cb50-11"><a href="#cb50-11"></a>    generated_model.layers, vanilla_nerf.field_coarse.mlp_base.layers</span>
<span id="cb50-12"><a href="#cb50-12"></a>)</span>
<span id="cb50-13"><a href="#cb50-13"></a></span>
<span id="cb50-14"><a href="#cb50-14"></a></span>
<span id="cb50-15"><a href="#cb50-15"></a>loggable_dict <span class="op">=</span> mlp_blocks_searcher.loggable_dict</span>
<span id="cb50-16"><a href="#cb50-16"></a><span class="co"># initial_blockspecs_to_reproduce_vanilla: MLPBlockSpecsConfig = (</span></span>
<span id="cb50-17"><a href="#cb50-17"></a><span class="co">#     mlp_blocks_searcher.get_blockspecs_config(BaseMLPArchStyle.VANILLA)</span></span>
<span id="cb50-18"><a href="#cb50-18"></a><span class="co"># )</span></span>
<span id="cb50-19"><a href="#cb50-19"></a><span class="co"># print(initial_blockspecs_to_reproduce_vanilla)</span></span>
<span id="cb50-20"><a href="#cb50-20"></a></span>
<span id="cb50-21"><a href="#cb50-21"></a><span class="co"># base_mlp_to_reproduce_vanilla = mlp_blocks_searcher.generate_model(</span></span>
<span id="cb50-22"><a href="#cb50-22"></a><span class="co">#     initial_blockspecs_to_reproduce_vanilla.setup()</span></span>
<span id="cb50-23"><a href="#cb50-23"></a><span class="co"># )</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1"></a>baseline_flops <span class="op">=</span> get_flops(vanilla_nerf.field_coarse.mlp_base, input_shape)</span>
<span id="cb51-2"><a href="#cb51-2"></a>baseline_params <span class="op">=</span> get_params(vanilla_nerf.field_coarse.mlp_base)</span>
<span id="cb51-3"><a href="#cb51-3"></a></span>
<span id="cb51-4"><a href="#cb51-4"></a>gen_nerf_base_mlp_flops <span class="op">=</span> get_flops(generated_model, input_shape)</span>
<span id="cb51-5"><a href="#cb51-5"></a>gen_nerf_base_mlp_params <span class="op">=</span> get_params(generated_model)</span>
<span id="cb51-6"><a href="#cb51-6"></a></span>
<span id="cb51-7"><a href="#cb51-7"></a><span class="co"># Validate that GenNerf MLP Base matches VanillaNerf MLP Base flops and params</span></span>
<span id="cb51-8"><a href="#cb51-8"></a><span class="cf">assert</span> (</span>
<span id="cb51-9"><a href="#cb51-9"></a>    baseline_flops <span class="op">==</span> gen_nerf_base_mlp_flops</span>
<span id="cb51-10"><a href="#cb51-10"></a>), <span class="ss">f"GenNerf MLP Base flops </span><span class="sc">{</span>gen_nerf_base_mlp_flops<span class="sc">}</span><span class="ss"> != VanillaNerf MLP Base flops </span><span class="sc">{</span>baseline_flops<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb51-11"><a href="#cb51-11"></a></span>
<span id="cb51-12"><a href="#cb51-12"></a><span class="cf">assert</span> (</span>
<span id="cb51-13"><a href="#cb51-13"></a>    baseline_params <span class="op">==</span> gen_nerf_base_mlp_params</span>
<span id="cb51-14"><a href="#cb51-14"></a>), <span class="ss">f"GenNerf MLP Base params </span><span class="sc">{</span>gen_nerf_base_mlp_params<span class="sc">}</span><span class="ss"> != VanillaNerf MLP Base params </span><span class="sc">{</span>baseline_params<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb51-15"><a href="#cb51-15"></a></span>
<span id="cb51-16"><a href="#cb51-16"></a><span class="bu">print</span>(<span class="ss">f"GenNerf MLP Base matches VanillaNerf MLP Base!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>GenNerf MLP Base matches VanillaNerf MLP Base!</code></pre>
</div>
</div>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1"></a>loggable_dict</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>{'generated': True,
 'target_ratio': 1.0,
 'build_metric': &lt;BuildMetrics.FLOPS: 1&gt;,
 'pretrained': False,
 'arch_style': &lt;BaseMLPArchStyle.LARGE_UNIFORM: 'LARGE_UNIFORM'&gt;,
 'generated_mlp': {'params': 493056,
  'flops': 491008,
  'attributes': {'stage1': {'channels': 256,
    'depth': 4,
    'freeze_depth': False,
    'freeze_channels': False},
   'stage2': {'channels': 256,
    'depth': 1,
    'freeze_depth': True,
    'freeze_channels': False},
   'stage3': {'channels': 256,
    'depth': 3,
    'freeze_depth': False,
    'freeze_channels': False},
   'skip_connections': [4]}},
 'baseline_vanilla_mlp': {'params': 493056,
  'flops': 491008,
  'attributes': {'stage1': {'channels': 256,
    'depth': 4,
    'freeze_depth': False,
    'freeze_channels': False},
   'stage2': {'channels': 256,
    'depth': 1,
    'freeze_depth': True,
    'freeze_channels': False},
   'stage3': {'channels': 256,
    'depth': 3,
    'freeze_depth': False,
    'freeze_channels': False},
   'skip_connections': [4]}},
 'flop_ratio_vs_baseline_vanilla': 1.0,
 'params_ratio_vs_baseline_vanilla': 1.0,
 'baseline_initial_mlp': {'params': 493056,
  'flops': 491008,
  'attributes': {'stage1': {'channels': 256,
    'depth': 4,
    'freeze_depth': False,
    'freeze_channels': False},
   'stage2': {'channels': 256,
    'depth': 1,
    'freeze_depth': True,
    'freeze_channels': False},
   'stage3': {'channels': 256,
    'depth': 3,
    'freeze_depth': False,
    'freeze_channels': False},
   'skip_connections': [4]}},
 'flop_ratio_vs_baseline_initial': 1.0,
 'params_ratio_vs_baseline_initial': 1.0}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="33">
<div class="cell-output cell-output-display">
<div class="sourceCode" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1"></a><span class="kw">class</span> MLPBlockSpecs(BlockSpecs):</span>
<span id="cb55-2"><a href="#cb55-2"></a>    <span class="co">"""A list of BlockSpecs for the MLPs in NeRF.</span></span>
<span id="cb55-3"><a href="#cb55-3"></a><span class="co">    This class can be directly passed to `darwinai.torch.builder.build_model()` as</span></span>
<span id="cb55-4"><a href="#cb55-4"></a><span class="co">    it satisfies the requirement of being a `Sequence[BlockSpec]`.</span></span>
<span id="cb55-5"><a href="#cb55-5"></a></span>
<span id="cb55-6"><a href="#cb55-6"></a><span class="co">    Args:</span></span>
<span id="cb55-7"><a href="#cb55-7"></a><span class="co">        stage1: First stage of the base MLP in a NeRF Field.</span></span>
<span id="cb55-8"><a href="#cb55-8"></a><span class="co">        stage2: Second stage of the base MLP in a NeRF Field.</span></span>
<span id="cb55-9"><a href="#cb55-9"></a><span class="co">        stage3: Third stage of the base MLP in a NeRF Field.</span></span>
<span id="cb55-10"><a href="#cb55-10"></a></span>
<span id="cb55-11"><a href="#cb55-11"></a><span class="co">    Raises:</span></span>
<span id="cb55-12"><a href="#cb55-12"></a><span class="co">        ValueError: If any of the BlockSpecs are None.</span></span>
<span id="cb55-13"><a href="#cb55-13"></a><span class="co">    """</span></span>
<span id="cb55-14"><a href="#cb55-14"></a></span>
<span id="cb55-15"><a href="#cb55-15"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, stage1: BlockSpec, stage2: BlockSpec, stage3: BlockSpec):</span>
<span id="cb55-16"><a href="#cb55-16"></a>        <span class="cf">if</span> stage1 <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> stage2 <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> stage3 <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb55-17"><a href="#cb55-17"></a>            <span class="bu">super</span>().<span class="fu">__init__</span>([stage1, stage2, stage3])</span>
<span id="cb55-18"><a href="#cb55-18"></a>        <span class="cf">else</span>:</span>
<span id="cb55-19"><a href="#cb55-19"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"BlockSpec for stage 1, 2, or 3 cannot be None."</span>)</span>
<span id="cb55-20"><a href="#cb55-20"></a></span>
<span id="cb55-21"><a href="#cb55-21"></a>    <span class="at">@property</span></span>
<span id="cb55-22"><a href="#cb55-22"></a>    <span class="kw">def</span> stage1(<span class="va">self</span>):</span>
<span id="cb55-23"><a href="#cb55-23"></a>        <span class="cf">return</span> <span class="va">self</span>[<span class="dv">0</span>]</span>
<span id="cb55-24"><a href="#cb55-24"></a></span>
<span id="cb55-25"><a href="#cb55-25"></a>    <span class="at">@property</span></span>
<span id="cb55-26"><a href="#cb55-26"></a>    <span class="kw">def</span> stage2(<span class="va">self</span>):</span>
<span id="cb55-27"><a href="#cb55-27"></a>        <span class="cf">return</span> <span class="va">self</span>[<span class="dv">1</span>]</span>
<span id="cb55-28"><a href="#cb55-28"></a></span>
<span id="cb55-29"><a href="#cb55-29"></a>    <span class="at">@property</span></span>
<span id="cb55-30"><a href="#cb55-30"></a>    <span class="kw">def</span> stage3(<span class="va">self</span>):</span>
<span id="cb55-31"><a href="#cb55-31"></a>        <span class="cf">return</span> <span class="va">self</span>[<span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div class="cell" data-execution_count="34">
<div class="cell-output cell-output-display">
<div class="sourceCode" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1"></a><span class="kw">class</span> BlockSpecs(UserList[BlockSpec]):</span>
<span id="cb56-2"><a href="#cb56-2"></a>    <span class="co">"""A pure wrapper around a list of BlockSpec objects.</span></span>
<span id="cb56-3"><a href="#cb56-3"></a></span>
<span id="cb56-4"><a href="#cb56-4"></a><span class="co">    This class satisfies the `Sequence[BlockSpec]` bound for `TBlockSpecs`,</span></span>
<span id="cb56-5"><a href="#cb56-5"></a><span class="co">    so it can be passed to `darwinai.torch.builder.build_model()`. All architectures</span></span>
<span id="cb56-6"><a href="#cb56-6"></a><span class="co">    that can be dynamically generated based on BlockSpec configurations should</span></span>
<span id="cb56-7"><a href="#cb56-7"></a><span class="co">    subclass this class.</span></span>
<span id="cb56-8"><a href="#cb56-8"></a></span>
<span id="cb56-9"><a href="#cb56-9"></a><span class="co">    The constructor for this class is flexible. It can be passed a single sequence</span></span>
<span id="cb56-10"><a href="#cb56-10"></a><span class="co">    of BlockSpec objects (such as a list or tuple), like this:</span></span>
<span id="cb56-11"><a href="#cb56-11"></a></span>
<span id="cb56-12"><a href="#cb56-12"></a><span class="co">        BlockSpecs([stage1, stage2, stage3])</span></span>
<span id="cb56-13"><a href="#cb56-13"></a></span>
<span id="cb56-14"><a href="#cb56-14"></a><span class="co">    Alternatively, it can be passed one or more BlockSpec objects directly, like this:</span></span>
<span id="cb56-15"><a href="#cb56-15"></a></span>
<span id="cb56-16"><a href="#cb56-16"></a><span class="co">        BlockSpecs(stage1, stage2, stage3)</span></span>
<span id="cb56-17"><a href="#cb56-17"></a></span>
<span id="cb56-18"><a href="#cb56-18"></a><span class="co">    In the latter case, the BlockSpec arguments are collected into a list in the</span></span>
<span id="cb56-19"><a href="#cb56-19"></a><span class="co">    order they were passed.</span></span>
<span id="cb56-20"><a href="#cb56-20"></a></span>
<span id="cb56-21"><a href="#cb56-21"></a><span class="co">    Args:</span></span>
<span id="cb56-22"><a href="#cb56-22"></a><span class="co">        blockspecs: One or more BlockSpec objects, or a single sequence of BlockSpec objects.</span></span>
<span id="cb56-23"><a href="#cb56-23"></a><span class="co">    """</span></span>
<span id="cb56-24"><a href="#cb56-24"></a></span>
<span id="cb56-25"><a href="#cb56-25"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>blockspecs: Union[BlockSpec, Sequence[BlockSpec]]):</span>
<span id="cb56-26"><a href="#cb56-26"></a>        flat_blockspecs: List[BlockSpec] <span class="op">=</span> []</span>
<span id="cb56-27"><a href="#cb56-27"></a>        <span class="cf">for</span> item <span class="kw">in</span> blockspecs:</span>
<span id="cb56-28"><a href="#cb56-28"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(item, Sequence):</span>
<span id="cb56-29"><a href="#cb56-29"></a>                flat_blockspecs.extend(item)</span>
<span id="cb56-30"><a href="#cb56-30"></a>            <span class="cf">else</span>:</span>
<span id="cb56-31"><a href="#cb56-31"></a>                flat_blockspecs.append(item)</span>
<span id="cb56-32"><a href="#cb56-32"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(flat_blockspecs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div class="cell" data-execution_count="35">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1"></a>show_doc(MLPBlockSpecs, title_level<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="35">
<hr>
<section id="mlpblockspecs" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="mlpblockspecs"><span class="header-section-number">5.4.2</span> MLPBlockSpecs</h3>
<blockquote class="blockquote">
<pre><code> MLPBlockSpecs (stage1:darwinai.torch.builder.BlockSpec,
                stage2:darwinai.torch.builder.BlockSpec,
                stage3:darwinai.torch.builder.BlockSpec)</code></pre>
</blockquote>
<p>A list of BlockSpecs for the MLPs in NeRF. This class can be directly passed to <code>darwinai.torch.builder.build_model()</code> as it satisfies the requirement of being a <code>Sequence[BlockSpec]</code>.</p>
<p>Args: stage1: First stage of the base MLP in a NeRF Field. stage2: Second stage of the base MLP in a NeRF Field. stage3: Third stage of the base MLP in a NeRF Field.</p>
<p>Raises: ValueError: If any of the BlockSpecs are None.</p>
</section>
</div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1"></a>show_doc(BlockSpecs, title_level<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<hr>
<section id="blockspecs" class="level3" data-number="5.4.3">
<h3 data-number="5.4.3" class="anchored" data-anchor-id="blockspecs"><span class="header-section-number">5.4.3</span> BlockSpecs</h3>
<blockquote class="blockquote">
<pre><code> BlockSpecs (*blockspecs:Union[darwinai.torch.builder.BlockSpec,Sequence[d
             arwinai.torch.builder.BlockSpec]])</code></pre>
</blockquote>
<p>A pure wrapper around a list of BlockSpec objects.</p>
<p>This class satisfies the <code>Sequence[BlockSpec]</code> bound for <code>TBlockSpecs</code>, so it can be passed to <code>darwinai.torch.builder.build_model()</code>. All architectures that can be dynamically generated based on BlockSpec configurations should subclass this class.</p>
<p>The constructor for this class is flexible. It can be passed a single sequence of BlockSpec objects (such as a list or tuple), like this:</p>
<pre><code>BlockSpecs([stage1, stage2, stage3])</code></pre>
<p>Alternatively, it can be passed one or more BlockSpec objects directly, like this:</p>
<pre><code>BlockSpecs(stage1, stage2, stage3)</code></pre>
<p>In the latter case, the BlockSpec arguments are collected into a list in the order they were passed.</p>
<p>Args: blockspecs: One or more BlockSpec objects, or a single sequence of BlockSpec objects.</p>
</section>
</div>
</div>
</section>
</section>
<section id="generate-the-blockspecs-produced-by-darwinai-sdk-for-the-w23nerf-model" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="generate-the-blockspecs-produced-by-darwinai-sdk-for-the-w23nerf-model"><span class="header-section-number">5.5</span> Generate the BlockSpecs produced by DarwinAI SDK for the W23NeRF model</h2>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1"></a><span class="kw">def</span> get_darwin_params(make_model_fn, ratio, initial_blockspec, input_shape<span class="op">=</span>[<span class="dv">63</span> <span class="op">+</span> <span class="dv">27</span>]):</span>
<span id="cb63-2"><a href="#cb63-2"></a>    <span class="co"># Darwin Builder</span></span>
<span id="cb63-3"><a href="#cb63-3"></a>    target_flops_ratio <span class="op">=</span> ratio</span>
<span id="cb63-4"><a href="#cb63-4"></a></span>
<span id="cb63-5"><a href="#cb63-5"></a>    model <span class="op">=</span> build_model(</span>
<span id="cb63-6"><a href="#cb63-6"></a>        make_model_fn,</span>
<span id="cb63-7"><a href="#cb63-7"></a>        initial_blockspec,</span>
<span id="cb63-8"><a href="#cb63-8"></a>        input_shape,</span>
<span id="cb63-9"><a href="#cb63-9"></a>        target_flops_ratio,</span>
<span id="cb63-10"><a href="#cb63-10"></a>        pretrained_model<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb63-11"><a href="#cb63-11"></a>    )</span>
<span id="cb63-12"><a href="#cb63-12"></a></span>
<span id="cb63-13"><a href="#cb63-13"></a>    <span class="cf">return</span> model, model.Ws, model.D</span>
<span id="cb63-14"><a href="#cb63-14"></a></span>
<span id="cb63-15"><a href="#cb63-15"></a></span>
<span id="cb63-16"><a href="#cb63-16"></a><span class="kw">def</span> test_flop_ratios_w23(</span>
<span id="cb63-17"><a href="#cb63-17"></a>    initial_blockspec,</span>
<span id="cb63-18"><a href="#cb63-18"></a>    flop_ratios<span class="op">=</span>[<span class="fl">1.0</span>, <span class="fl">0.9</span>, <span class="fl">0.8</span>, <span class="fl">0.7</span>, <span class="fl">0.6</span>, <span class="fl">0.5</span>, <span class="fl">0.4</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>, <span class="fl">0.1</span>, <span class="fl">0.05</span>],</span>
<span id="cb63-19"><a href="#cb63-19"></a>):</span>
<span id="cb63-20"><a href="#cb63-20"></a>    initial_blockspec_str <span class="op">=</span> [(bs.channels, bs.depth) <span class="cf">for</span> bs <span class="kw">in</span> initial_blockspec]</span>
<span id="cb63-21"><a href="#cb63-21"></a></span>
<span id="cb63-22"><a href="#cb63-22"></a>    input_shape <span class="op">=</span> [<span class="dv">63</span> <span class="op">+</span> <span class="dv">27</span>]</span>
<span id="cb63-23"><a href="#cb63-23"></a></span>
<span id="cb63-24"><a href="#cb63-24"></a>    <span class="cf">if</span> <span class="bu">len</span>(<span class="bu">set</span>([bs.channels <span class="cf">for</span> bs <span class="kw">in</span> initial_blockspec])) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb63-25"><a href="#cb63-25"></a>        <span class="bu">print</span>(</span>
<span id="cb63-26"><a href="#cb63-26"></a>            <span class="ss">f"Initializing each stage to have the same layer width for model type W23NeRF."</span></span>
<span id="cb63-27"><a href="#cb63-27"></a>        )</span>
<span id="cb63-28"><a href="#cb63-28"></a>    <span class="cf">else</span>:</span>
<span id="cb63-29"><a href="#cb63-29"></a>        <span class="bu">print</span>(</span>
<span id="cb63-30"><a href="#cb63-30"></a>            <span class="ss">f"Initializing each stage to have different layer widths for model type W23NeRF"</span></span>
<span id="cb63-31"><a href="#cb63-31"></a>        )</span>
<span id="cb63-32"><a href="#cb63-32"></a></span>
<span id="cb63-33"><a href="#cb63-33"></a>    baseline_model <span class="op">=</span> make_model_w23(initial_blockspec)</span>
<span id="cb63-34"><a href="#cb63-34"></a>    baseline_flops <span class="op">=</span> get_flops(baseline_model, input_shape)</span>
<span id="cb63-35"><a href="#cb63-35"></a>    baseline_params <span class="op">=</span> get_params(baseline_model)</span>
<span id="cb63-36"><a href="#cb63-36"></a>    <span class="bu">print</span>(</span>
<span id="cb63-37"><a href="#cb63-37"></a>        <span class="ss">f"Using initial blockspecs (List[BlockSpec(channel, depth)]): </span><span class="ch">\n\t</span><span class="sc">{</span>initial_blockspec_str<span class="sc">}</span><span class="ss">."</span></span>
<span id="cb63-38"><a href="#cb63-38"></a>    )</span>
<span id="cb63-39"><a href="#cb63-39"></a></span>
<span id="cb63-40"><a href="#cb63-40"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="st">'Target FR'</span><span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Actual FR'</span><span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Actual PR'</span><span class="sc">:&lt;10}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'Ws'</span><span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span><span class="st">'D'</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb63-41"><a href="#cb63-41"></a>    <span class="cf">for</span> fr <span class="kw">in</span> flop_ratios:</span>
<span id="cb63-42"><a href="#cb63-42"></a>        model <span class="op">=</span> build_model(</span>
<span id="cb63-43"><a href="#cb63-43"></a>            make_model_w23,</span>
<span id="cb63-44"><a href="#cb63-44"></a>            initial_blockspec,</span>
<span id="cb63-45"><a href="#cb63-45"></a>            input_shape,</span>
<span id="cb63-46"><a href="#cb63-46"></a>            fr,</span>
<span id="cb63-47"><a href="#cb63-47"></a>            pretrained_model<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb63-48"><a href="#cb63-48"></a>        )</span>
<span id="cb63-49"><a href="#cb63-49"></a>        new_flops <span class="op">=</span> get_flops(model, input_shape)</span>
<span id="cb63-50"><a href="#cb63-50"></a>        new_params <span class="op">=</span> get_params(model)</span>
<span id="cb63-51"><a href="#cb63-51"></a>        Ws_string <span class="op">=</span> <span class="bu">tuple</span>(model.Ws[:<span class="dv">3</span>])</span>
<span id="cb63-52"><a href="#cb63-52"></a></span>
<span id="cb63-53"><a href="#cb63-53"></a>        <span class="co"># Compute new flop ratio and params ratio.</span></span>
<span id="cb63-54"><a href="#cb63-54"></a>        flops_ratio <span class="op">=</span> new_flops <span class="op">/</span> baseline_flops</span>
<span id="cb63-55"><a href="#cb63-55"></a>        params_ratio <span class="op">=</span> new_params <span class="op">/</span> baseline_params</span>
<span id="cb63-56"><a href="#cb63-56"></a></span>
<span id="cb63-57"><a href="#cb63-57"></a>        <span class="bu">print</span>(</span>
<span id="cb63-58"><a href="#cb63-58"></a>            <span class="ss">f"</span><span class="sc">{</span>fr<span class="sc">:&lt;10.2f}</span><span class="ss"> </span><span class="sc">{</span>flops_ratio<span class="sc">:&lt;10.2f}</span><span class="ss"> </span><span class="sc">{</span>params_ratio<span class="sc">:&lt;10.2f}</span><span class="ss"> </span><span class="sc">{</span><span class="bu">str</span>(Ws_string)<span class="sc">:&lt;20}</span><span class="ss"> </span><span class="sc">{</span>model<span class="sc">.</span>D<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb63-59"><a href="#cb63-59"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="blockspecs-generated-by-buildsdk-for-the-eddy-vanilla-nerf-with-default256" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="blockspecs-generated-by-buildsdk-for-the-eddy-vanilla-nerf-with-default256"><span class="header-section-number">5.5.1</span> BlockSpecs generated by BuildSDK for the Eddy Vanilla NeRF with default=256</h3>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1"></a>default <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb64-2"><a href="#cb64-2"></a></span>
<span id="cb64-3"><a href="#cb64-3"></a><span class="co"># Same initial widths.</span></span>
<span id="cb64-4"><a href="#cb64-4"></a>initial_blockspec <span class="op">=</span> [</span>
<span id="cb64-5"><a href="#cb64-5"></a>    BlockSpec(default, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb64-6"><a href="#cb64-6"></a>    BlockSpec(default, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb64-7"><a href="#cb64-7"></a>    BlockSpec(default, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb64-8"><a href="#cb64-8"></a>]</span>
<span id="cb64-9"><a href="#cb64-9"></a>test_flop_ratios_w23(initial_blockspec)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Initializing each stage to have the same layer width for model type W23NeRF.
Using initial blockspecs (List[BlockSpec(channel, depth)]): 
    [(256, 8), (256, 8), (256, 8)].
Target FR  Actual FR  Actual PR  Ws                   D
1.00       1.00       1.00       (256, 256, 256)      8
0.90       0.87       0.87       (253, 253, 253)      7
0.80       0.82       0.82       (245, 245, 245)      7
0.70       0.73       0.73       (231, 231, 231)      7
0.60       0.60       0.60       (224, 224, 224)      6
0.50       0.52       0.52       (208, 208, 208)      6
0.40       0.38       0.38       (191, 191, 191)      5
0.30       0.28       0.28       (177, 177, 177)      4
0.20       0.20       0.20       (149, 149, 149)      4
0.10       0.10       0.10       (102, 102, 102)      4
0.05       0.05       0.05       (65, 65, 65)         4</code></pre>
</div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1"></a><span class="co"># Different initial widths.</span></span>
<span id="cb66-2"><a href="#cb66-2"></a>initial_blockspec <span class="op">=</span> [</span>
<span id="cb66-3"><a href="#cb66-3"></a>    BlockSpec(<span class="dv">240</span>, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb66-4"><a href="#cb66-4"></a>    BlockSpec(<span class="dv">248</span>, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb66-5"><a href="#cb66-5"></a>    BlockSpec(<span class="dv">256</span>, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb66-6"><a href="#cb66-6"></a>]</span>
<span id="cb66-7"><a href="#cb66-7"></a>test_flop_ratios(initial_blockspec, <span class="st">"w23"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>NameError: name 'test_flop_ratios' is not defined</code></pre>
</div>
</div>
</section>
<section id="blockspecs-generated-by-buildsdk-for-the-eddy-vanilla-nerf-with-default64" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="blockspecs-generated-by-buildsdk-for-the-eddy-vanilla-nerf-with-default64"><span class="header-section-number">5.5.2</span> BlockSpecs generated by BuildSDK for the Eddy Vanilla NeRF with default=64</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1"></a>default <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb68-2"><a href="#cb68-2"></a></span>
<span id="cb68-3"><a href="#cb68-3"></a><span class="co"># Same initial widths.</span></span>
<span id="cb68-4"><a href="#cb68-4"></a>initial_blockspec <span class="op">=</span> [</span>
<span id="cb68-5"><a href="#cb68-5"></a>    BlockSpec(default, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb68-6"><a href="#cb68-6"></a>    BlockSpec(default, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb68-7"><a href="#cb68-7"></a>    BlockSpec(default, <span class="dv">8</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb68-8"><a href="#cb68-8"></a>]</span>
<span id="cb68-9"><a href="#cb68-9"></a>test_flop_ratios(initial_blockspec, <span class="st">"w23"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Initializing each stage to have the same layer width for model type eddy.
Using initial blockspecs (List[BlockSpec(channel, depth)]): 
    [(64, 8), (64, 8), (64, 8)].
Target FR  Actual FR  Actual PR  Ws                   D
1.00       1.00       1.00       (64, 64, 64)         8
0.90       0.86       0.86       (62, 62, 62)         7
0.80       0.76       0.76       (58, 58, 58)         7
0.70       0.66       0.66       (57, 57, 57)         6
0.60       0.59       0.59       (53, 53, 53)         6
0.50       0.49       0.49       (51, 51, 51)         5
0.40       0.40       0.40       (45, 45, 45)         5
0.30       0.29       0.29       (40, 40, 40)         4
0.20       0.21       0.21       (32, 32, 32)         4
0.10       0.10       0.10       (19, 19, 19)         4
0.05       0.06       0.06       (12, 12, 12)         5</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1"></a><span class="co"># Different initial widths.</span></span>
<span id="cb70-2"><a href="#cb70-2"></a>initial_blockspec <span class="op">=</span> [</span>
<span id="cb70-3"><a href="#cb70-3"></a>    BlockSpec(<span class="dv">56</span>, <span class="dv">4</span>, <span class="va">False</span>, <span class="va">True</span>),</span>
<span id="cb70-4"><a href="#cb70-4"></a>    BlockSpec(<span class="dv">60</span>, <span class="dv">4</span>, <span class="va">False</span>, <span class="va">True</span>),</span>
<span id="cb70-5"><a href="#cb70-5"></a>    BlockSpec(<span class="dv">64</span>, <span class="dv">6</span>, <span class="va">False</span>, <span class="va">False</span>),</span>
<span id="cb70-6"><a href="#cb70-6"></a>]</span>
<span id="cb70-7"><a href="#cb70-7"></a>test_flop_ratios(initial_blockspec, <span class="st">"w23"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Initializing each stage to have different layer widths for model type eddy
Using initial blockspecs (List[BlockSpec(channel, depth)]): 
    [(56, 4), (60, 4), (64, 6)].
Target FR  Actual FR  Actual PR  Ws                   D
1.00       1.00       1.00       (56, 60, 64)         6
0.90       0.88       0.88       (55, 59, 63)         5
0.80       0.77       0.77       (51, 55, 58)         5
0.70       0.71       0.71       (48, 52, 56)         5
0.60       0.59       0.59       (47, 51, 54)         4
0.50       0.52       0.52       (44, 47, 50)         4
0.40       0.38       0.38       (38, 41, 45)         3
0.30       0.31       0.31       (33, 36, 39)         3
0.20       0.20       0.20       (24, 26, 29)         3
0.10       0.10       0.10       (14, 16, 18)         3
0.05       0.06       0.06       (9, 11, 12)          3</code></pre>
</div>
</div>
</section>
<section id="blockspecs-generated-by-buildsdk-for-the-vanilla-gennerf-with-default256" class="level3" data-number="5.5.3">
<h3 data-number="5.5.3" class="anchored" data-anchor-id="blockspecs-generated-by-buildsdk-for-the-vanilla-gennerf-with-default256"><span class="header-section-number">5.5.3</span> BlockSpecs generated by BuildSDK for the Vanilla GenNeRF with default=256</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1"></a><span class="co"># Same initial widths.</span></span>
<span id="cb72-2"><a href="#cb72-2"></a>stage1 <span class="op">=</span> BlockSpec(<span class="dv">256</span>, <span class="dv">4</span>, <span class="va">False</span>, <span class="va">False</span>)</span>
<span id="cb72-3"><a href="#cb72-3"></a>stage2 <span class="op">=</span> BlockSpec(<span class="dv">256</span>, <span class="dv">1</span>, <span class="va">False</span>, <span class="va">False</span>)</span>
<span id="cb72-4"><a href="#cb72-4"></a>stage3 <span class="op">=</span> BlockSpec(<span class="dv">256</span>, <span class="dv">3</span>, <span class="va">False</span>, <span class="va">False</span>)</span>
<span id="cb72-5"><a href="#cb72-5"></a></span>
<span id="cb72-6"><a href="#cb72-6"></a>initial_blockspec <span class="op">=</span> GenNerfBaseMLPBlocks(stage1, stage2, stage3)</span>
<span id="cb72-7"><a href="#cb72-7"></a>test_flop_ratios(initial_blockspec, <span class="st">"gen_nerf"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Initializing each stage to have the same layer width for model type gen_nerf.
Using initial blockspecs (List[BlockSpec(channel, depth)]): 
    [(256, 4), (256, 1), (256, 3)].
Target FR  Actual FR  Actual PR  Ws                   D
1.00       1.00       1.00       (256, 256, 256)      [4, 1, 3]
0.90       0.73       0.73       (255, 255, 255)      [3, 1, 2]
0.80       0.73       0.73       (255, 255, 255)      [3, 1, 2]
0.70       0.67       0.67       (245, 245, 245)      [3, 1, 2]
0.60       0.62       0.62       (235, 235, 235)      [3, 1, 2]
0.50       0.52       0.52       (214, 214, 214)      [3, 1, 2]
0.40       0.39       0.39       (203, 203, 203)      [2, 1, 2]
0.30       0.33       0.33       (186, 186, 186)      [2, 1, 2]
0.20       0.20       0.20       (162, 162, 162)      [2, 1, 1]
0.10       0.10       0.10       (109, 109, 109)      [2, 1, 1]
0.05       0.05       0.05       (74, 74, 74)         [2, 1, 1]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1"></a><span class="co"># Different initial widths.</span></span>
<span id="cb74-2"><a href="#cb74-2"></a>stage1 <span class="op">=</span> BlockSpec(<span class="dv">240</span>, <span class="dv">4</span>, <span class="va">False</span>, <span class="va">False</span>)</span>
<span id="cb74-3"><a href="#cb74-3"></a>stage2 <span class="op">=</span> BlockSpec(<span class="dv">248</span>, <span class="dv">1</span>, <span class="va">False</span>, <span class="va">False</span>)</span>
<span id="cb74-4"><a href="#cb74-4"></a>stage3 <span class="op">=</span> BlockSpec(<span class="dv">256</span>, <span class="dv">3</span>, <span class="va">False</span>, <span class="va">False</span>)</span>
<span id="cb74-5"><a href="#cb74-5"></a></span>
<span id="cb74-6"><a href="#cb74-6"></a>initial_blockspec <span class="op">=</span> GenNerfBaseMLPBlocks(stage1, stage2, stage3)</span>
<span id="cb74-7"><a href="#cb74-7"></a>test_flop_ratios(initial_blockspec, <span class="st">"gen_nerf"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Initializing each stage to have different layer widths for model type gen_nerf
Using initial blockspecs (List[BlockSpec(channel, depth)]): 
    [(240, 4), (248, 1), (256, 3)].
Target FR  Actual FR  Actual PR  Ws                   D
1.00       1.00       1.00       (240, 248, 256)      [4, 1, 3]
0.90       0.73       0.73       (239, 247, 255)      [3, 1, 2]
0.80       0.73       0.73       (239, 247, 255)      [3, 1, 2]
0.70       0.68       0.68       (230, 238, 245)      [3, 1, 2]
0.60       0.62       0.62       (220, 227, 235)      [3, 1, 2]
0.50       0.52       0.52       (200, 207, 214)      [3, 1, 2]
0.40       0.39       0.40       (189, 196, 203)      [2, 1, 2]
0.30       0.34       0.34       (174, 180, 186)      [2, 1, 2]
0.20       0.20       0.20       (150, 156, 162)      [2, 1, 1]
0.10       0.10       0.10       (100, 104, 109)      [2, 1, 1]
0.05       0.05       0.05       (66, 70, 74)         [2, 1, 1]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1"></a><span class="co"># Same initial widths.</span></span>
<span id="cb76-2"><a href="#cb76-2"></a>stage1 <span class="op">=</span> BlockSpec(<span class="dv">240</span>, <span class="dv">4</span>, <span class="va">False</span>, <span class="va">False</span>)</span>
<span id="cb76-3"><a href="#cb76-3"></a>stage2 <span class="op">=</span> BlockSpec(<span class="dv">248</span>, <span class="dv">1</span>, <span class="va">False</span>, <span class="va">True</span>)</span>
<span id="cb76-4"><a href="#cb76-4"></a>stage3 <span class="op">=</span> BlockSpec(<span class="dv">256</span>, <span class="dv">3</span>, <span class="va">False</span>, <span class="va">False</span>)</span>
<span id="cb76-5"><a href="#cb76-5"></a></span>
<span id="cb76-6"><a href="#cb76-6"></a>initial_blockspec <span class="op">=</span> GenNerfBaseMLPBlocks(stage1, stage2, stage3)</span>
<span id="cb76-7"><a href="#cb76-7"></a>test_flop_ratios(initial_blockspec, <span class="st">"gen_nerf"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Initializing each stage to have different layer widths for model type gen_nerf
Using initial blockspecs (List[BlockSpec(channel, depth)]): 
    [(240, 4), (248, 1), (256, 3)].
Target FR  Actual FR  Actual PR  Ws                   D
1.00       1.00       1.00       (240, 248, 256)      [4, 1, 3]
0.90       0.73       0.73       (239, 247, 255)      [3, 1, 2]
0.80       0.73       0.73       (239, 247, 255)      [3, 1, 2]
0.70       0.68       0.68       (230, 238, 245)      [3, 1, 2]
0.60       0.62       0.62       (220, 227, 235)      [3, 1, 2]
0.50       0.52       0.52       (200, 207, 214)      [3, 1, 2]
0.40       0.39       0.40       (189, 196, 203)      [2, 1, 2]
0.30       0.34       0.34       (174, 180, 186)      [2, 1, 2]
0.20       0.20       0.20       (150, 156, 162)      [2, 1, 1]
0.10       0.10       0.10       (100, 104, 109)      [2, 1, 1]
0.05       0.05       0.05       (66, 70, 74)         [2, 1, 1]</code></pre>
</div>
</div>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>